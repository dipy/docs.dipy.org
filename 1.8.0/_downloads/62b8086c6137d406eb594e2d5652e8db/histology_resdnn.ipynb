{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Local reconstruction using the Histological ResDNN\n\nA data-driven approach to modeling the non-linear mapping between observed\nDW-MRI signals and ground truth structures using sequential deep neural network\nregression with residual block deep neural network (ResDNN) [1, 2].\n\nTraining was performed on two 3-D histology datasets of squirrel monkey brains\nand validated on a third. A second validation was performed on HCP datasets.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import os\n\nimport numpy as np\nimport scipy.ndimage as ndi\n\nfrom dipy.core.gradients import gradient_table\nfrom dipy.data import get_fnames, get_sphere\nfrom dipy.io.image import load_nifti, save_nifti\nfrom dipy.io.gradients import read_bvals_bvecs\nfrom dipy.nn.histo_resdnn import HistoResDNN\nfrom dipy.reconst.shm import sh_to_sf_matrix\nfrom dipy.segment.mask import median_otsu\nfrom dipy.viz import window, actor\n\n\n# Disable oneDNN warning\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This ResDNN model requires single-shell data with one or more b0s, the data\nis fetched and a gradient table is constructed from bvals/bvecs.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Fetch DWI and GTAB\nhardi_fname, hardi_bval_fname, hardi_bvec_fname = get_fnames('stanford_hardi')\ndata, affine = load_nifti(hardi_fname)\ndata = np.squeeze(data)\nbvals, bvecs = read_bvals_bvecs(hardi_bval_fname, hardi_bvec_fname)\ngtab = gradient_table(bvals, bvecs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To accelerate computation, a brain mask must be computed. The resulting mask\nis saved for visual inspection.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "mean_b0 = data[..., gtab.b0s_mask]\nmean_b0 = np.mean(mean_b0, axis=-1)\n_, mask = median_otsu(mean_b0)\n\nmask_labeled, _ = ndi.label(mask)\nunique, count = np.unique(mask_labeled, return_counts=True)\nval = unique[np.argmax(count[1:])+1]\nmask[mask_labeled != val] = 0\n\nsave_nifti('mask.nii.gz', mask.astype(np.uint8), affine)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Using a ResDNN for sh_order 8 (default) and load the appropriate weights.\nFit the data and save the resulting fODF.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "resdnn_model = HistoResDNN(verbose=True)\nresdnn_model.fetch_default_weights()\npredicted_sh = resdnn_model.predict(data, gtab, mask=mask)\n\nsave_nifti('predicted_sh.nii.gz', predicted_sh, affine)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Preparing the scene using FURY. The ODF slicer and the background image are\nadded as actors and a mid-coronal slice is selected.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "interactive = False\nsphere = get_sphere('repulsion724')\nB, invB = sh_to_sf_matrix(sphere, sh_order=8,\n                          basis_type=resdnn_model.basis_type, smooth=0.0006)\nfod_spheres = actor.odf_slicer(predicted_sh, sphere=sphere,\n                               scale=0.6, norm=True, mask=mask, B_matrix=B)\n\nmean_sh = np.mean(predicted_sh, axis=-1)\nbackground_img = actor.slicer(mean_sh, opacity=0.5,\n                              interpolation='nearest')\n\n# Select the mid coronal slice for slicer\nori_shape = mask.shape[0:3]\nslice_index = ori_shape[1] // 2\nfod_spheres.display_extent(0, ori_shape[0],\n                           slice_index, slice_index,\n                           0, ori_shape[2])\nbackground_img.display_extent(0, ori_shape[0],\n                              slice_index, slice_index,\n                              0, ori_shape[2])\n\nscene = window.Scene()\nscene.add(fod_spheres)\nscene.add(background_img)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Adjusting the camera for a nice zoom-in of the corpus callosum (coronal) to\nscreenshot the resulting prediction. FODF should be aligned with the\ncurvature of the corpus callosum and a single-fiber population should be\nvisible.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "camera = {\n    'zoom_factor': 0.85,\n    'view_position': np.array([\n        (ori_shape[0] - 1) / 2.0,\n        max(ori_shape),\n        (ori_shape[2] - 1) / 1.5]),\n    'view_center': np.array([\n        (ori_shape[0] - 1) / 2.0,\n        slice_index,\n        (ori_shape[2] - 1) / 1.5]),\n    'up_vector': np.array([0.0, 0.0, 1.0]),\n}\n\nscene.set_camera(position=camera['view_position'],\n                 focal_point=camera['view_center'],\n                 view_up=camera['up_vector'])\nscene.zoom(camera['zoom_factor'])\n\nif interactive:\n    window.show(scene, reset_camera=False)\n\nwindow.record(scene, out_path='pred_fODF.png', size=(1000, 1000),\n              reset_camera=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ".. rst-class:: centered small fst-italic fw-semibold\n\nVisualization of the predicted fODF.\n\n\n\n## References\n..  [1] Nath, V., Schilling, K. G., Parvathaneni, P., Hansen,\n    C. B., Hainline, A. E., Huo, Y., ... & Stepniewska, I. (2019).\n    Deep learning reveals untapped information for local white-matter\n    fiber reconstruction in diffusion-weighted MRI.\n    Magnetic resonance imaging, 62, 220-227.\n..  [2] Nath, V., Schilling, K. G., Hansen, C. B., Parvathaneni,\n    P., Hainline, A. E., Bermudez, C., ... & St\u0119pniewska, I. (2019).\n    Deep learning captures more accurate diffusion fiber orientations\n    distributions than constrained spherical deconvolution.\n    arXiv preprint arXiv:1911.07927.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ".. include:: ../../links_names.inc\n\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}