
<!DOCTYPE html>


<html lang="en" data-content_root="../../../../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>DIPY &#8212; dipy 1.10.0.dev0+git20241107.f469878 documentation</title>
  
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../../../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../../../../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../../../../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../../../../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../../../../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../../../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../../../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../../../../_static/pygments.css?v=fa44fd50" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/css/dipy.css?v=cedb193d" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/sg_gallery.css?v=61a4c737" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/sg_gallery-binder.css?v=f4aeca0c" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/sg_gallery-dataframe.css?v=2082cf3c" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/sg_gallery-rendered-html.css?v=1277b6f3" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/design-style.1e8bd061cd6da7fc9cf755528e8ffc24.min.css?v=0a3b3ea7" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../../../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../../../../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

  
<link rel="stylesheet" href="../../../../_static/styles/grg-sphinx-theme.css"/>

    <script src="../../../../_static/documentation_options.js?v=977fc5d0"></script>
    <script src="../../../../_static/doctools.js?v=888ff710"></script>
    <script src="../../../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../../../../_static/design-tabs.js?v=36754332"></script>
    <script async="async" src="https://www.googletagmanager.com/gtag/js?id=G-D610GKJZRC"></script>
    <script>
                window.dataLayer = window.dataLayer || [];
                function gtag(){ dataLayer.push(arguments); }
                gtag('js', new Date());
                gtag('config', 'G-D610GKJZRC');
            </script>
    <script>
                window.dataLayer = window.dataLayer || [];
                function gtag(){ dataLayer.push(arguments); }
                gtag('js', new Date());
                gtag('config', 'G-D610GKJZRC');
            </script>
    <script>DOCUMENTATION_OPTIONS.pagename = '_modules/keras/src/layers/layer';</script>
    <script>
        DOCUMENTATION_OPTIONS.theme_version = '0.15.4';
        DOCUMENTATION_OPTIONS.theme_switcher_json_url = 'https://docs.dipy.org/dev/_static/version_switcher.json';
        DOCUMENTATION_OPTIONS.theme_switcher_version_match = 'development';
        DOCUMENTATION_OPTIONS.show_version_warning_banner = true;
        </script>
    <link rel="icon" href="../../../../_static/dipy-favicon.png"/>
    <link rel="index" title="Index" href="../../../../genindex.html" />
    <link rel="search" title="Search" href="../../../../search.html" />
  <meta name="keywords" content="DIPY, dMRI, DTI, DSI, diffusion MRI, Tensor,
  neuroimaging, python, neuroscience, Eleftherios, Garyfallidis, tractography,
  streamlines, fiber tracking">

  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../../../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search the docs ..."
         aria-label="Search the docs ..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
<nav class="bd-header navbar navbar-expand-lg bd-navbar">
<div class="bd-header__inner bd-page-width">
  <button class="pst-navbar-icon sidebar-toggle primary-toggle" aria-label="Site navigation">
    <span class="fa-solid fa-bars"></span>
  </button>
  
  
  <div class="col-lg-3 navbar-header-items__start">
    
      <div class="navbar-item">

  
     
  

<a class="navbar-brand logo" href="https://dipy.org">
  
  
  
  
  
    
    
      
    
    
    <img src="../../../../_static/dipy-logo.png" class="logo__image only-light" alt="DIPY"/>
    <script>document.write(`<img src="../../../../_static/dipy-logo.png" class="logo__image only-dark" alt="DIPY"/>`);</script>
  
  
</a></div>
    
  </div>
  
  <div class="col-lg-9 navbar-header-items">
    
    <div class="me-auto navbar-header-items__center">
      
        <div class="navbar-item"><nav class="navbar-nav">
  <p class="sidebar-header-items__title"
     role="heading"
     aria-level="1"
     aria-label="Site Navigation">
    Site Navigation
  </p>
  
  <ul class="bd-navbar-elements navbar-nav">
    
          <li class="nav-item dropdown">
                <button class="btn dropdown-toggle nav-item" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-controls="pst-header-nav-more-links">
                    Docs
                </button>
                <ul id="pst-header-nav-more-links" class="dropdown-menu">
                    
      <li class="nav-item">
        <a class="nav-link" href="../../../../index.html">
          Overview
        </a>
      </li>

      <li class="nav-item">
        <a class="nav-link" href="../../../../examples_built/index.html">
          Tutorials
        </a>
      </li>

      <li class="nav-item">
        <a class="nav-link" href="../../../../recipes.html">
          Recipes
        </a>
      </li>

      <li class="nav-item">
        <a class="nav-link" href="../../../../interfaces/index.html">
          CLI / Workflows
        </a>
      </li>

      <li class="nav-item">
        <a class="nav-link" href="../../../../reference/index.html">
          API
        </a>
      </li>

      <li class="nav-item">
        <a class="nav-link" href="../../../../reference_cmd/index.html">
          CLI API
        </a>
      </li>
                </ul>
            </li>
          

          <li class="nav-item dropdown">
                <button class="btn dropdown-toggle nav-item" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-controls="pst-header-nav-more-links">
                    Workshops
                </button>
                <ul id="pst-header-nav-more-links" class="dropdown-menu">
                    
    <li class="nav-item">
      <p class="nav-section-title nav-link">
        Latest
      </p>
    </li>

        <li class="nav-item">
          <a class="nav-link" href="https://dipy.org/workshops/dipy-workshop-2024" target="_blank">
            DIPY Workshop 2024 <i class="fa-solid fa-arrow-up-long external-icon mar-l-5"></i>
          </a>
        </li>

    <li class="nav-item">
      <p class="nav-section-title nav-link">
        Past
      </p>
    </li>

        <li class="nav-item">
          <a class="nav-link" href="https://dipy.org/workshops/dipy-workshop-2023" target="_blank">
            DIPY Workshop 2023 <i class="fa-solid fa-arrow-up-long external-icon mar-l-5"></i>
          </a>
        </li>

        <li class="nav-item">
          <a class="nav-link" href="https://dipy.org/workshops/dipy-workshop-2022" target="_blank">
            DIPY Workshop 2022 <i class="fa-solid fa-arrow-up-long external-icon mar-l-5"></i>
          </a>
        </li>

        <li class="nav-item">
          <a class="nav-link" href="https://dipy.org/workshops/dipy-workshop-2021" target="_blank">
            DIPY Workshop 2021 <i class="fa-solid fa-arrow-up-long external-icon mar-l-5"></i>
          </a>
        </li>

        <li class="nav-item">
          <a class="nav-link" href="https://dipy.org/workshops/dipy-workshop-2020" target="_blank">
            DIPY Workshop 2020 <i class="fa-solid fa-arrow-up-long external-icon mar-l-5"></i>
          </a>
        </li>

        <li class="nav-item">
          <a class="nav-link" href="https://dipy.org/workshops/dipy-workshop-2019" target="_blank">
            DIPY Workshop 2019 <i class="fa-solid fa-arrow-up-long external-icon mar-l-5"></i>
          </a>
        </li>
                </ul>
            </li>
          

          <li class="nav-item dropdown">
                <button class="btn dropdown-toggle nav-item" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-controls="pst-header-nav-more-links">
                    Community
                </button>
                <ul id="pst-header-nav-more-links" class="dropdown-menu">
                    
    <li class="nav-item">
      <p class="nav-section-title nav-link">
        News
      </p>
    </li>

        <li class="nav-item">
          <a class="nav-link" href="https://dipy.org/calendar">
            Calendar
          </a>
        </li>

        <li class="nav-item">
          <a class="nav-link" href="https://mail.python.org/mailman3/lists/dipy.python.org/" target="_blank">
            Newsletters <i class="fa-solid fa-arrow-up-long external-icon mar-l-5"></i>
          </a>
        </li>

        <li class="nav-item">
          <a class="nav-link" href="https://dipy.org/blog">
            Blog
          </a>
        </li>

        <li class="nav-item">
          <a class="nav-link" href="https://www.youtube.com/c/diffusionimaginginpython" target="_blank">
            Youtube <i class="fa-solid fa-arrow-up-long external-icon mar-l-5"></i>
          </a>
        </li>

    <li class="nav-item">
      <p class="nav-section-title nav-link">
        Help
      </p>
    </li>

        <li class="nav-item">
          <a class="nav-link" href="https://app.gitter.im/#/room/%23dipy_dipy:gitter.im" target="_blank">
            Live Chat (Gitter) <i class="fa-solid fa-arrow-up-long external-icon mar-l-5"></i>
          </a>
        </li>

        <li class="nav-item">
          <a class="nav-link" href="https://github.com/dipy/dipy/discussions" target="_blank">
            Github Discussions <i class="fa-solid fa-arrow-up-long external-icon mar-l-5"></i>
          </a>
        </li>
                </ul>
            </li>
          

          <li class="nav-item dropdown">
                <button class="btn dropdown-toggle nav-item" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-controls="pst-header-nav-more-links">
                    About
                </button>
                <ul id="pst-header-nav-more-links" class="dropdown-menu">
                    
        <li class="nav-item">
          <a class="nav-link" href="https://dipy.org/team">
            Team
          </a>
        </li>

      <li class="nav-item">
        <a class="nav-link" href="../../../../faq.html">
          FAQ
        </a>
      </li>

      <li class="nav-item">
        <a class="nav-link" href="../../../../user_guide/mission.html">
          Mission Statement
        </a>
      </li>

      <li class="nav-item">
        <a class="nav-link" href="../../../../stateoftheart.html">
          Releases
        </a>
      </li>

      <li class="nav-item">
        <a class="nav-link" href="../../../../cite.html">
          Cite
        </a>
      </li>

      <li class="nav-item">
        <a class="nav-link" href="../../../../glossary.html">
          Glossary
        </a>
      </li>
                </ul>
            </li>
          
  </ul>
  
</nav></div>
      
    </div>
    
    
    <div class="navbar-header-items__end">
      
      
        <div class="navbar-item">
<form class="bd-search d-flex align-items-center"
      action="../../../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search the docs ..."
         aria-label="Search the docs ..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
      
        <div class="navbar-item">
<script>
document.write(`
  <div class="version-switcher__container dropdown">
    <button id="pst-version-switcher-button-2"
      type="button"
      class="version-switcher__button btn btn-sm dropdown-toggle"
      data-bs-toggle="dropdown"
      aria-haspopup="listbox"
      aria-controls="pst-version-switcher-list-2"
      aria-label="Version switcher list"
    >
      Choose version  <!-- this text may get changed later by javascript -->
      <span class="caret"></span>
    </button>
    <div id="pst-version-switcher-list-2"
      class="version-switcher__menu dropdown-menu list-group-flush py-0"
      role="listbox" aria-labelledby="pst-version-switcher-button-2">
      <!-- dropdown will be populated by javascript on page load -->
    </div>
  </div>
`);
</script></div>
      
        <div class="navbar-item"><ul class="navbar-icon-links"
    aria-label="Icon Links">
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://github.com/dipy" title="GitHub" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-github fa-lg" aria-hidden="true"></i>
            <span class="sr-only">GitHub</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://twitter.com/dipymri" title="Twitter/X" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-twitter fa-lg" aria-hidden="true"></i>
            <span class="sr-only">Twitter/X</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://www.youtube.com/c/diffusionimaginginpython" title="YouTube" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-youtube fa-lg" aria-hidden="true"></i>
            <span class="sr-only">YouTube</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://www.linkedin.com/company/dipy/" title="LinkedIn" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-linkedin fa-lg" aria-hidden="true"></i>
            <span class="sr-only">LinkedIn</span></a>
        </li>
</ul></div>
      
    </div>
    
  </div>
  
  

  
</div>

</nav>

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
        
      
      <div class="bd-sidebar-primary bd-sidebar hide-on-wide">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
      <div class="sidebar-header-items__center">
        
          
          
            <div class="navbar-item"><nav class="navbar-nav">
  <p class="sidebar-header-items__title"
     role="heading"
     aria-level="1"
     aria-label="Site Navigation">
    Site Navigation
  </p>
  
  <ul class="bd-navbar-elements navbar-nav">
    
          <li class="nav-item dropdown">
                <button class="btn dropdown-toggle nav-item" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-controls="pst-header-nav-more-links">
                    Docs
                </button>
                <ul id="pst-header-nav-more-links" class="dropdown-menu">
                    
      <li class="nav-item">
        <a class="nav-link" href="../../../../index.html">
          Overview
        </a>
      </li>

      <li class="nav-item">
        <a class="nav-link" href="../../../../examples_built/index.html">
          Tutorials
        </a>
      </li>

      <li class="nav-item">
        <a class="nav-link" href="../../../../recipes.html">
          Recipes
        </a>
      </li>

      <li class="nav-item">
        <a class="nav-link" href="../../../../interfaces/index.html">
          CLI / Workflows
        </a>
      </li>

      <li class="nav-item">
        <a class="nav-link" href="../../../../reference/index.html">
          API
        </a>
      </li>

      <li class="nav-item">
        <a class="nav-link" href="../../../../reference_cmd/index.html">
          CLI API
        </a>
      </li>
                </ul>
            </li>
          

          <li class="nav-item dropdown">
                <button class="btn dropdown-toggle nav-item" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-controls="pst-header-nav-more-links">
                    Workshops
                </button>
                <ul id="pst-header-nav-more-links" class="dropdown-menu">
                    
    <li class="nav-item">
      <p class="nav-section-title nav-link">
        Latest
      </p>
    </li>

        <li class="nav-item">
          <a class="nav-link" href="https://dipy.org/workshops/dipy-workshop-2024" target="_blank">
            DIPY Workshop 2024 <i class="fa-solid fa-arrow-up-long external-icon mar-l-5"></i>
          </a>
        </li>

    <li class="nav-item">
      <p class="nav-section-title nav-link">
        Past
      </p>
    </li>

        <li class="nav-item">
          <a class="nav-link" href="https://dipy.org/workshops/dipy-workshop-2023" target="_blank">
            DIPY Workshop 2023 <i class="fa-solid fa-arrow-up-long external-icon mar-l-5"></i>
          </a>
        </li>

        <li class="nav-item">
          <a class="nav-link" href="https://dipy.org/workshops/dipy-workshop-2022" target="_blank">
            DIPY Workshop 2022 <i class="fa-solid fa-arrow-up-long external-icon mar-l-5"></i>
          </a>
        </li>

        <li class="nav-item">
          <a class="nav-link" href="https://dipy.org/workshops/dipy-workshop-2021" target="_blank">
            DIPY Workshop 2021 <i class="fa-solid fa-arrow-up-long external-icon mar-l-5"></i>
          </a>
        </li>

        <li class="nav-item">
          <a class="nav-link" href="https://dipy.org/workshops/dipy-workshop-2020" target="_blank">
            DIPY Workshop 2020 <i class="fa-solid fa-arrow-up-long external-icon mar-l-5"></i>
          </a>
        </li>

        <li class="nav-item">
          <a class="nav-link" href="https://dipy.org/workshops/dipy-workshop-2019" target="_blank">
            DIPY Workshop 2019 <i class="fa-solid fa-arrow-up-long external-icon mar-l-5"></i>
          </a>
        </li>
                </ul>
            </li>
          

          <li class="nav-item dropdown">
                <button class="btn dropdown-toggle nav-item" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-controls="pst-header-nav-more-links">
                    Community
                </button>
                <ul id="pst-header-nav-more-links" class="dropdown-menu">
                    
    <li class="nav-item">
      <p class="nav-section-title nav-link">
        News
      </p>
    </li>

        <li class="nav-item">
          <a class="nav-link" href="https://dipy.org/calendar">
            Calendar
          </a>
        </li>

        <li class="nav-item">
          <a class="nav-link" href="https://mail.python.org/mailman3/lists/dipy.python.org/" target="_blank">
            Newsletters <i class="fa-solid fa-arrow-up-long external-icon mar-l-5"></i>
          </a>
        </li>

        <li class="nav-item">
          <a class="nav-link" href="https://dipy.org/blog">
            Blog
          </a>
        </li>

        <li class="nav-item">
          <a class="nav-link" href="https://www.youtube.com/c/diffusionimaginginpython" target="_blank">
            Youtube <i class="fa-solid fa-arrow-up-long external-icon mar-l-5"></i>
          </a>
        </li>

    <li class="nav-item">
      <p class="nav-section-title nav-link">
        Help
      </p>
    </li>

        <li class="nav-item">
          <a class="nav-link" href="https://app.gitter.im/#/room/%23dipy_dipy:gitter.im" target="_blank">
            Live Chat (Gitter) <i class="fa-solid fa-arrow-up-long external-icon mar-l-5"></i>
          </a>
        </li>

        <li class="nav-item">
          <a class="nav-link" href="https://github.com/dipy/dipy/discussions" target="_blank">
            Github Discussions <i class="fa-solid fa-arrow-up-long external-icon mar-l-5"></i>
          </a>
        </li>
                </ul>
            </li>
          

          <li class="nav-item dropdown">
                <button class="btn dropdown-toggle nav-item" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-controls="pst-header-nav-more-links">
                    About
                </button>
                <ul id="pst-header-nav-more-links" class="dropdown-menu">
                    
        <li class="nav-item">
          <a class="nav-link" href="https://dipy.org/team">
            Team
          </a>
        </li>

      <li class="nav-item">
        <a class="nav-link" href="../../../../faq.html">
          FAQ
        </a>
      </li>

      <li class="nav-item">
        <a class="nav-link" href="../../../../user_guide/mission.html">
          Mission Statement
        </a>
      </li>

      <li class="nav-item">
        <a class="nav-link" href="../../../../stateoftheart.html">
          Releases
        </a>
      </li>

      <li class="nav-item">
        <a class="nav-link" href="../../../../cite.html">
          Cite
        </a>
      </li>

      <li class="nav-item">
        <a class="nav-link" href="../../../../glossary.html">
          Glossary
        </a>
      </li>
                </ul>
            </li>
          
  </ul>
  
</nav></div>
          
        
      </div>
    
    
    
      <div class="sidebar-header-items__end">
        
          <div class="navbar-item">
<form class="bd-search d-flex align-items-center"
      action="../../../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search the docs ..."
         aria-label="Search the docs ..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
        
          <div class="navbar-item">
<script>
document.write(`
  <div class="version-switcher__container dropdown">
    <button id="pst-version-switcher-button-3"
      type="button"
      class="version-switcher__button btn btn-sm dropdown-toggle"
      data-bs-toggle="dropdown"
      aria-haspopup="listbox"
      aria-controls="pst-version-switcher-list-3"
      aria-label="Version switcher list"
    >
      Choose version  <!-- this text may get changed later by javascript -->
      <span class="caret"></span>
    </button>
    <div id="pst-version-switcher-list-3"
      class="version-switcher__menu dropdown-menu list-group-flush py-0"
      role="listbox" aria-labelledby="pst-version-switcher-button-3">
      <!-- dropdown will be populated by javascript on page load -->
    </div>
  </div>
`);
</script></div>
        
          <div class="navbar-item"><ul class="navbar-icon-links"
    aria-label="Icon Links">
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://github.com/dipy" title="GitHub" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-github fa-lg" aria-hidden="true"></i>
            <span class="sr-only">GitHub</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://twitter.com/dipymri" title="Twitter/X" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-twitter fa-lg" aria-hidden="true"></i>
            <span class="sr-only">Twitter/X</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://www.youtube.com/c/diffusionimaginginpython" title="YouTube" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-youtube fa-lg" aria-hidden="true"></i>
            <span class="sr-only">YouTube</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://www.linkedin.com/company/dipy/" title="LinkedIn" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-linkedin fa-lg" aria-hidden="true"></i>
            <span class="sr-only">LinkedIn</span></a>
        </li>
</ul></div>
        
      </div>
    
  </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        
          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item">



<nav aria-label="Breadcrumb" class="d-print-none">
  <ul class="bd-breadcrumbs">
    
    <li class="breadcrumb-item breadcrumb-home">
      <a href="../../../../index.html" class="nav-link" aria-label="Home">
        <i class="fa-solid fa-home"></i>
      </a>
    </li>
    
    <li class="breadcrumb-item"><a href="../../../index.html" class="nav-link">Module code</a></li>
    
    <li class="breadcrumb-item active" aria-current="page">DIPY</li>
  </ul>
</nav>
</div>
      
    </div>
  
  
</div>
</div>
              
              
              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <h1>Source code for keras.src.layers.layer</h1><div class="highlight"><pre>
<span></span><span class="sd">&quot;&quot;&quot;Layer is an Operation with state.</span>

<span class="sd">Takes care of:</span>

<span class="sd">- Weights / variables (and tracking thereof)</span>
<span class="sd">- deferred build</span>
<span class="sd">- trainable argument value inference</span>
<span class="sd">- masking</span>
<span class="sd">- autocasting</span>

<span class="sd">And some more magic:</span>

<span class="sd">- add_loss</span>
<span class="sd">- metric tracking</span>
<span class="sd">- RNG seed tracking</span>
<span class="sd">- activity regularization</span>
<span class="sd">&quot;&quot;&quot;</span>

<span class="kn">import</span> <span class="nn">collections</span>
<span class="kn">import</span> <span class="nn">inspect</span>
<span class="kn">import</span> <span class="nn">warnings</span>
<span class="kn">from</span> <span class="nn">functools</span> <span class="kn">import</span> <span class="n">wraps</span>

<span class="kn">from</span> <span class="nn">keras.src</span> <span class="kn">import</span> <span class="n">backend</span>
<span class="kn">from</span> <span class="nn">keras.src</span> <span class="kn">import</span> <span class="n">constraints</span>
<span class="kn">from</span> <span class="nn">keras.src</span> <span class="kn">import</span> <span class="n">dtype_policies</span>
<span class="kn">from</span> <span class="nn">keras.src</span> <span class="kn">import</span> <span class="n">initializers</span>
<span class="kn">from</span> <span class="nn">keras.src</span> <span class="kn">import</span> <span class="n">regularizers</span>
<span class="kn">from</span> <span class="nn">keras.src</span> <span class="kn">import</span> <span class="n">tree</span>
<span class="kn">from</span> <span class="nn">keras.src</span> <span class="kn">import</span> <span class="n">utils</span>
<span class="kn">from</span> <span class="nn">keras.src.api_export</span> <span class="kn">import</span> <span class="n">keras_export</span>
<span class="kn">from</span> <span class="nn">keras.src.backend</span> <span class="kn">import</span> <span class="n">KerasTensor</span>
<span class="kn">from</span> <span class="nn">keras.src.backend.common</span> <span class="kn">import</span> <span class="n">global_state</span>
<span class="kn">from</span> <span class="nn">keras.src.backend.common.name_scope</span> <span class="kn">import</span> <span class="n">current_path</span>
<span class="kn">from</span> <span class="nn">keras.src.backend.common.symbolic_scope</span> <span class="kn">import</span> <span class="n">in_symbolic_scope</span>
<span class="kn">from</span> <span class="nn">keras.src.distribution</span> <span class="kn">import</span> <span class="n">distribution_lib</span>
<span class="kn">from</span> <span class="nn">keras.src.dtype_policies</span> <span class="kn">import</span> <span class="n">DTypePolicyMap</span>
<span class="kn">from</span> <span class="nn">keras.src.layers</span> <span class="kn">import</span> <span class="n">input_spec</span>
<span class="kn">from</span> <span class="nn">keras.src.metrics.metric</span> <span class="kn">import</span> <span class="n">Metric</span>
<span class="kn">from</span> <span class="nn">keras.src.ops.operation</span> <span class="kn">import</span> <span class="n">Operation</span>
<span class="kn">from</span> <span class="nn">keras.src.saving.keras_saveable</span> <span class="kn">import</span> <span class="n">KerasSaveable</span>
<span class="kn">from</span> <span class="nn">keras.src.utils</span> <span class="kn">import</span> <span class="n">python_utils</span>
<span class="kn">from</span> <span class="nn">keras.src.utils</span> <span class="kn">import</span> <span class="n">summary_utils</span>
<span class="kn">from</span> <span class="nn">keras.src.utils</span> <span class="kn">import</span> <span class="n">traceback_utils</span>
<span class="kn">from</span> <span class="nn">keras.src.utils</span> <span class="kn">import</span> <span class="n">tracking</span>

<span class="k">if</span> <span class="n">backend</span><span class="o">.</span><span class="n">backend</span><span class="p">()</span> <span class="o">==</span> <span class="s2">&quot;tensorflow&quot;</span><span class="p">:</span>
    <span class="kn">from</span> <span class="nn">keras.src.backend.tensorflow.layer</span> <span class="kn">import</span> <span class="n">TFLayer</span> <span class="k">as</span> <span class="n">BackendLayer</span>
<span class="k">elif</span> <span class="n">backend</span><span class="o">.</span><span class="n">backend</span><span class="p">()</span> <span class="o">==</span> <span class="s2">&quot;jax&quot;</span><span class="p">:</span>
    <span class="kn">from</span> <span class="nn">keras.src.backend.jax.layer</span> <span class="kn">import</span> <span class="n">JaxLayer</span> <span class="k">as</span> <span class="n">BackendLayer</span>
<span class="k">elif</span> <span class="n">backend</span><span class="o">.</span><span class="n">backend</span><span class="p">()</span> <span class="o">==</span> <span class="s2">&quot;torch&quot;</span><span class="p">:</span>
    <span class="kn">from</span> <span class="nn">keras.src.backend.torch.layer</span> <span class="kn">import</span> <span class="n">TorchLayer</span> <span class="k">as</span> <span class="n">BackendLayer</span>
<span class="k">elif</span> <span class="n">backend</span><span class="o">.</span><span class="n">backend</span><span class="p">()</span> <span class="o">==</span> <span class="s2">&quot;numpy&quot;</span><span class="p">:</span>
    <span class="kn">from</span> <span class="nn">keras.src.backend.numpy.layer</span> <span class="kn">import</span> <span class="n">NumpyLayer</span> <span class="k">as</span> <span class="n">BackendLayer</span>
<span class="k">else</span><span class="p">:</span>
    <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
        <span class="sa">f</span><span class="s2">&quot;Backend &#39;</span><span class="si">{</span><span class="n">backend</span><span class="o">.</span><span class="n">backend</span><span class="p">()</span><span class="si">}</span><span class="s2">&#39; must implement a layer mixin class.&quot;</span>
    <span class="p">)</span>


<span class="nd">@keras_export</span><span class="p">([</span><span class="s2">&quot;keras.Layer&quot;</span><span class="p">,</span> <span class="s2">&quot;keras.layers.Layer&quot;</span><span class="p">])</span>
<span class="k">class</span> <span class="nc">Layer</span><span class="p">(</span><span class="n">BackendLayer</span><span class="p">,</span> <span class="n">Operation</span><span class="p">,</span> <span class="n">KerasSaveable</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;This is the class from which all layers inherit.</span>

<span class="sd">    A layer is a callable object that takes as input one or more tensors and</span>
<span class="sd">    that outputs one or more tensors. It involves *computation*, defined</span>
<span class="sd">    in the `call()` method, and a *state* (weight variables). State can be</span>
<span class="sd">    created:</span>

<span class="sd">    * in `__init__()`, for instance via `self.add_weight()`;</span>
<span class="sd">    * in the optional `build()` method, which is invoked by the first</span>
<span class="sd">      `__call__()` to the layer, and supplies the shape(s) of the input(s),</span>
<span class="sd">      which may not have been known at initialization time.</span>

<span class="sd">    Layers are recursively composable: If you assign a Layer instance as an</span>
<span class="sd">    attribute of another Layer, the outer layer will start tracking the weights</span>
<span class="sd">    created by the inner layer. Nested layers should be instantiated in the</span>
<span class="sd">    `__init__()` method or `build()` method.</span>

<span class="sd">    Users will just instantiate a layer and then treat it as a callable.</span>

<span class="sd">    Args:</span>
<span class="sd">        trainable: Boolean, whether the layer&#39;s variables should be trainable.</span>
<span class="sd">        name: String name of the layer.</span>
<span class="sd">        dtype: The dtype of the layer&#39;s computations and weights. Can also be a</span>
<span class="sd">            `keras.DTypePolicy`,</span>
<span class="sd">            which allows the computation and</span>
<span class="sd">            weight dtype to differ. Defaults to `None`. `None` means to use</span>
<span class="sd">            `keras.config.dtype_policy()`,</span>
<span class="sd">            which is a `float32` policy unless set to different value</span>
<span class="sd">            (via `keras.config.set_dtype_policy()`).</span>

<span class="sd">    Attributes:</span>
<span class="sd">        name: The name of the layer (string).</span>
<span class="sd">        dtype: Dtype of the layer&#39;s weights. Alias of `layer.variable_dtype`.</span>
<span class="sd">        variable_dtype: Dtype of the layer&#39;s weights.</span>
<span class="sd">        compute_dtype: The dtype of the layer&#39;s computations.</span>
<span class="sd">            Layers automatically cast inputs to this dtype, which causes</span>
<span class="sd">            the computations and output to also be in this dtype.</span>
<span class="sd">            When mixed precision is used with a</span>
<span class="sd">            `keras.DTypePolicy`, this will be different</span>
<span class="sd">            than `variable_dtype`.</span>
<span class="sd">        trainable_weights: List of variables to be included in backprop.</span>
<span class="sd">        non_trainable_weights: List of variables that should not be</span>
<span class="sd">            included in backprop.</span>
<span class="sd">        weights: The concatenation of the lists trainable_weights and</span>
<span class="sd">            non_trainable_weights (in this order).</span>
<span class="sd">        trainable: Whether the layer should be trained (boolean), i.e.</span>
<span class="sd">            whether its potentially-trainable weights should be returned</span>
<span class="sd">            as part of `layer.trainable_weights`.</span>
<span class="sd">        input_spec: Optional (list of) `InputSpec` object(s) specifying the</span>
<span class="sd">            constraints on inputs that can be accepted by the layer.</span>

<span class="sd">    We recommend that descendants of `Layer` implement the following methods:</span>

<span class="sd">    * `__init__()`: Defines custom layer attributes, and creates layer weights</span>
<span class="sd">        that do not depend on input shapes, using `add_weight()`,</span>
<span class="sd">        or other state.</span>
<span class="sd">    * `build(self, input_shape)`: This method can be used to create weights that</span>
<span class="sd">        depend on the shape(s) of the input(s), using `add_weight()`, or other</span>
<span class="sd">        state. `__call__()` will automatically build the layer</span>
<span class="sd">        (if it has not been built yet) by calling `build()`.</span>
<span class="sd">    * `call(self, *args, **kwargs)`: Called in `__call__` after making</span>
<span class="sd">        sure `build()` has been called. `call()` performs the logic of applying</span>
<span class="sd">        the layer to the input arguments.</span>
<span class="sd">        Two reserved keyword arguments you can optionally use in `call()` are:</span>
<span class="sd">            1. `training` (boolean, whether the call is in inference mode or</span>
<span class="sd">                training mode).</span>
<span class="sd">            2. `mask` (boolean tensor encoding masked timesteps in the input,</span>
<span class="sd">                used e.g. in RNN layers).</span>
<span class="sd">        A typical signature for this method is `call(self, inputs)`, and user</span>
<span class="sd">        could optionally add `training` and `mask` if the layer need them.</span>
<span class="sd">    * `get_config(self)`: Returns a dictionary containing the configuration</span>
<span class="sd">        used to initialize this layer. If the keys differ from the arguments</span>
<span class="sd">        in `__init__()`, then override `from_config(self)` as well.</span>
<span class="sd">        This method is used when saving</span>
<span class="sd">        the layer or a model that contains this layer.</span>

<span class="sd">    Examples:</span>

<span class="sd">    Here&#39;s a basic example: a layer with two variables, `w` and `b`,</span>
<span class="sd">    that returns `y = w . x + b`.</span>
<span class="sd">    It shows how to implement `build()` and `call()`.</span>
<span class="sd">    Variables set as attributes of a layer are tracked as weights</span>
<span class="sd">    of the layers (in `layer.weights`).</span>

<span class="sd">    ```python</span>
<span class="sd">    class SimpleDense(Layer):</span>
<span class="sd">        def __init__(self, units=32):</span>
<span class="sd">            super().__init__()</span>
<span class="sd">            self.units = units</span>

<span class="sd">        # Create the state of the layer (weights)</span>
<span class="sd">        def build(self, input_shape):</span>
<span class="sd">            self.kernel = self.add_weight(</span>
<span class="sd">                shape=(input_shape[-1], self.units),</span>
<span class="sd">                initializer=&quot;glorot_uniform&quot;,</span>
<span class="sd">                trainable=True,</span>
<span class="sd">                name=&quot;kernel&quot;,</span>
<span class="sd">            )</span>
<span class="sd">            self.bias = self.add_weight(</span>
<span class="sd">                shape=(self.units,),</span>
<span class="sd">                initializer=&quot;zeros&quot;,</span>
<span class="sd">                trainable=True,</span>
<span class="sd">                name=&quot;bias&quot;,</span>
<span class="sd">            )</span>

<span class="sd">        # Defines the computation</span>
<span class="sd">        def call(self, inputs):</span>
<span class="sd">            return ops.matmul(inputs, self.kernel) + self.bias</span>

<span class="sd">    # Instantiates the layer.</span>
<span class="sd">    linear_layer = SimpleDense(4)</span>

<span class="sd">    # This will also call `build(input_shape)` and create the weights.</span>
<span class="sd">    y = linear_layer(ops.ones((2, 2)))</span>
<span class="sd">    assert len(linear_layer.weights) == 2</span>

<span class="sd">    # These weights are trainable, so they&#39;re listed in `trainable_weights`:</span>
<span class="sd">    assert len(linear_layer.trainable_weights) == 2</span>
<span class="sd">    ```</span>

<span class="sd">    Besides trainable weights, updated via backpropagation during training,</span>
<span class="sd">    layers can also have non-trainable weights. These weights are meant to</span>
<span class="sd">    be updated manually during `call()`. Here&#39;s a example layer that computes</span>
<span class="sd">    the running sum of its inputs:</span>

<span class="sd">    ```python</span>
<span class="sd">    class ComputeSum(Layer):</span>

<span class="sd">      def __init__(self, input_dim):</span>
<span class="sd">          super(ComputeSum, self).__init__()</span>
<span class="sd">          # Create a non-trainable weight.</span>
<span class="sd">          self.total = self.add_weight(</span>
<span class="sd">            shape=(),</span>
<span class="sd">            initializer=&quot;zeros&quot;,</span>
<span class="sd">            trainable=False,</span>
<span class="sd">            name=&quot;total&quot;,</span>
<span class="sd">          )</span>

<span class="sd">      def call(self, inputs):</span>
<span class="sd">          self.total.assign(self.total + ops.sum(inputs))</span>
<span class="sd">          return self.total</span>

<span class="sd">    my_sum = ComputeSum(2)</span>
<span class="sd">    x = ops.ones((2, 2))</span>
<span class="sd">    y = my_sum(x)</span>

<span class="sd">    assert my_sum.weights == [my_sum.total]</span>
<span class="sd">    assert my_sum.non_trainable_weights == [my_sum.total]</span>
<span class="sd">    assert my_sum.trainable_weights == []</span>
<span class="sd">    ```</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__new__</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="n">obj</span> <span class="o">=</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__new__</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

        <span class="c1"># Wrap the user-provided `build` method in the `build_wrapper`</span>
        <span class="c1"># to add name scope support and serialization support.</span>
        <span class="n">original_build_method</span> <span class="o">=</span> <span class="n">obj</span><span class="o">.</span><span class="n">build</span>

        <span class="nd">@wraps</span><span class="p">(</span><span class="n">original_build_method</span><span class="p">)</span>
        <span class="k">def</span> <span class="nf">build_wrapper</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
            <span class="k">with</span> <span class="n">obj</span><span class="o">.</span><span class="n">_open_name_scope</span><span class="p">():</span>
                <span class="n">obj</span><span class="o">.</span><span class="n">_path</span> <span class="o">=</span> <span class="n">current_path</span><span class="p">()</span>
                <span class="n">original_build_method</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
            <span class="c1"># Record build config.</span>
            <span class="n">signature</span> <span class="o">=</span> <span class="n">inspect</span><span class="o">.</span><span class="n">signature</span><span class="p">(</span><span class="n">original_build_method</span><span class="p">)</span>
            <span class="n">obj</span><span class="o">.</span><span class="n">_build_shapes_dict</span> <span class="o">=</span> <span class="n">signature</span><span class="o">.</span><span class="n">bind</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span><span class="o">.</span><span class="n">arguments</span>
            <span class="c1"># Set built, post build actions, and lock state.</span>
            <span class="n">obj</span><span class="o">.</span><span class="n">built</span> <span class="o">=</span> <span class="kc">True</span>
            <span class="n">obj</span><span class="o">.</span><span class="n">_post_build</span><span class="p">()</span>
            <span class="n">obj</span><span class="o">.</span><span class="n">_lock_state</span><span class="p">()</span>

        <span class="n">obj</span><span class="o">.</span><span class="n">build</span> <span class="o">=</span> <span class="n">build_wrapper</span>

        <span class="c1"># Wrap the user-provided `quantize` method in the `quantize_wrapper`</span>
        <span class="c1"># to add tracker support.</span>
        <span class="n">original_quantize_method</span> <span class="o">=</span> <span class="n">obj</span><span class="o">.</span><span class="n">quantize</span>

        <span class="nd">@wraps</span><span class="p">(</span><span class="n">original_quantize_method</span><span class="p">)</span>
        <span class="k">def</span> <span class="nf">quantize_wrapper</span><span class="p">(</span><span class="n">mode</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
            <span class="n">obj</span><span class="o">.</span><span class="n">_check_quantize_args</span><span class="p">(</span><span class="n">mode</span><span class="p">,</span> <span class="n">obj</span><span class="o">.</span><span class="n">compute_dtype</span><span class="p">)</span>
            <span class="n">obj</span><span class="o">.</span><span class="n">_tracker</span><span class="o">.</span><span class="n">unlock</span><span class="p">()</span>
            <span class="k">try</span><span class="p">:</span>
                <span class="n">original_quantize_method</span><span class="p">(</span><span class="n">mode</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
            <span class="k">except</span> <span class="ne">Exception</span><span class="p">:</span>
                <span class="k">raise</span>
            <span class="k">finally</span><span class="p">:</span>
                <span class="n">obj</span><span class="o">.</span><span class="n">_tracker</span><span class="o">.</span><span class="n">lock</span><span class="p">()</span>

        <span class="n">obj</span><span class="o">.</span><span class="n">quantize</span> <span class="o">=</span> <span class="n">quantize_wrapper</span>

        <span class="k">return</span> <span class="n">obj</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">activity_regularizer</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">trainable</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">autocast</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="n">BackendLayer</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_lock</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="n">Operation</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">activity_regularizer</span> <span class="o">=</span> <span class="n">regularizers</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">activity_regularizer</span><span class="p">)</span>
        <span class="n">input_dim_arg</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;input_dim&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">input_dim_arg</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">input_shape_arg</span> <span class="o">=</span> <span class="p">(</span><span class="n">input_dim_arg</span><span class="p">,)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">input_shape_arg</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;input_shape&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">input_shape_arg</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
                <span class="s2">&quot;Do not pass an `input_shape`/`input_dim` argument to &quot;</span>
                <span class="s2">&quot;a layer. When using Sequential models, &quot;</span>
                <span class="s2">&quot;prefer using an `Input(shape)` object as the &quot;</span>
                <span class="s2">&quot;first layer in the model instead.&quot;</span><span class="p">,</span>
                <span class="n">stacklevel</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_input_shape_arg</span> <span class="o">=</span> <span class="n">input_shape_arg</span>
        <span class="k">if</span> <span class="n">kwargs</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;Unrecognized keyword arguments &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;passed to </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">kwargs</span><span class="si">}</span><span class="s2">&quot;</span>
            <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_path</span> <span class="o">=</span> <span class="kc">None</span>  <span class="c1"># Will be determined in `build_wrapper`</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">built</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">autocast</span> <span class="o">=</span> <span class="n">autocast</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_input_spec</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_called</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">supports_jit</span> <span class="o">=</span> <span class="kc">True</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_trainable</span> <span class="o">=</span> <span class="n">trainable</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_losses</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_loss_ids</span> <span class="o">=</span> <span class="nb">set</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_losses_override</span> <span class="o">=</span> <span class="p">[]</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_call_signature</span> <span class="o">=</span> <span class="n">inspect</span><span class="o">.</span><span class="n">signature</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">call</span><span class="p">)</span>
        <span class="n">call_signature_parameters</span> <span class="o">=</span> <span class="p">[</span>
            <span class="n">p</span><span class="o">.</span><span class="n">name</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_call_signature</span><span class="o">.</span><span class="n">parameters</span><span class="o">.</span><span class="n">values</span><span class="p">()</span>
        <span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_call_has_training_arg</span> <span class="o">=</span> <span class="s2">&quot;training&quot;</span> <span class="ow">in</span> <span class="n">call_signature_parameters</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_call_has_mask_arg</span> <span class="o">=</span> <span class="s2">&quot;mask&quot;</span> <span class="ow">in</span> <span class="n">call_signature_parameters</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_supports_masking</span> <span class="o">=</span> <span class="ow">not</span> <span class="n">utils</span><span class="o">.</span><span class="n">is_default</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">compute_mask</span><span class="p">)</span>
        <span class="c1"># Whether to automatically convert (+ auto-cast) inputs to `call()`.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_convert_input_args</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="c1"># Whether to allow non-tensors as positional arguments in `call()`.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_allow_non_tensor_positional_args</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="c1"># Dict of shapes that were used to call `build()`.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_build_shapes_dict</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="c1"># Parent path</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_parent_path</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_initialize_tracker</span><span class="p">()</span>

    <span class="nd">@tracking</span><span class="o">.</span><span class="n">no_automatic_dependency_tracking</span>
    <span class="k">def</span> <span class="nf">_initialize_tracker</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;_tracker&quot;</span><span class="p">):</span>
            <span class="k">return</span>

        <span class="n">trainable_variables</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">non_trainable_variables</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">layers</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">metrics</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">seed_generators</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_tracker</span> <span class="o">=</span> <span class="n">tracking</span><span class="o">.</span><span class="n">Tracker</span><span class="p">(</span>
            <span class="p">{</span>
                <span class="s2">&quot;trainable_variables&quot;</span><span class="p">:</span> <span class="p">(</span>
                    <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">backend</span><span class="o">.</span><span class="n">Variable</span><span class="p">)</span> <span class="ow">and</span> <span class="n">x</span><span class="o">.</span><span class="n">trainable</span><span class="p">,</span>
                    <span class="n">trainable_variables</span><span class="p">,</span>
                <span class="p">),</span>
                <span class="s2">&quot;non_trainable_variables&quot;</span><span class="p">:</span> <span class="p">(</span>
                    <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">backend</span><span class="o">.</span><span class="n">Variable</span><span class="p">)</span>
                    <span class="ow">and</span> <span class="ow">not</span> <span class="n">x</span><span class="o">.</span><span class="n">trainable</span><span class="p">,</span>
                    <span class="n">non_trainable_variables</span><span class="p">,</span>
                <span class="p">),</span>
                <span class="s2">&quot;metrics&quot;</span><span class="p">:</span> <span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">Metric</span><span class="p">),</span> <span class="n">metrics</span><span class="p">),</span>
                <span class="s2">&quot;layers&quot;</span><span class="p">:</span> <span class="p">(</span>
                    <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">Layer</span><span class="p">)</span>
                    <span class="ow">and</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">Metric</span><span class="p">),</span>
                    <span class="n">layers</span><span class="p">,</span>
                <span class="p">),</span>
                <span class="s2">&quot;seed_generators&quot;</span><span class="p">:</span> <span class="p">(</span>
                    <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">backend</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">SeedGenerator</span><span class="p">),</span>
                    <span class="n">seed_generators</span><span class="p">,</span>
                <span class="p">),</span>
            <span class="p">},</span>
            <span class="n">exclusions</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;non_trainable_variables&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s2">&quot;trainable_variables&quot;</span><span class="p">]},</span>
        <span class="p">)</span>
        <span class="k">if</span> <span class="n">backend</span><span class="o">.</span><span class="n">backend</span><span class="p">()</span> <span class="o">==</span> <span class="s2">&quot;tensorflow&quot;</span><span class="p">:</span>
            <span class="c1"># Remove attribute tracking for lists (TF-specific attribute)</span>
            <span class="n">_self_setattr_tracking</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span>
                <span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;_self_setattr_tracking&quot;</span><span class="p">,</span> <span class="kc">True</span>
            <span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_self_setattr_tracking</span> <span class="o">=</span> <span class="kc">False</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_trainable_variables</span> <span class="o">=</span> <span class="n">trainable_variables</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_non_trainable_variables</span> <span class="o">=</span> <span class="n">non_trainable_variables</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_layers</span> <span class="o">=</span> <span class="n">layers</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_metrics</span> <span class="o">=</span> <span class="n">metrics</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_seed_generators</span> <span class="o">=</span> <span class="n">seed_generators</span>

        <span class="k">if</span> <span class="n">backend</span><span class="o">.</span><span class="n">backend</span><span class="p">()</span> <span class="o">==</span> <span class="s2">&quot;tensorflow&quot;</span><span class="p">:</span>
            <span class="c1"># Reset attribute tracking (TF-specific)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_self_setattr_tracking</span> <span class="o">=</span> <span class="n">_self_setattr_tracking</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">path</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;The path of the layer.</span>

<span class="sd">        If the layer has not been built yet, it will be `None`.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_path</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">input_spec</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_input_spec</span>

    <span class="nd">@input_spec</span><span class="o">.</span><span class="n">setter</span>
    <span class="k">def</span> <span class="nf">input_spec</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_input_spec</span> <span class="o">=</span> <span class="n">value</span>

    <span class="nd">@utils</span><span class="o">.</span><span class="n">default</span>
    <span class="k">def</span> <span class="nf">build</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_shape</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_check_super_called</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">utils</span><span class="o">.</span><span class="n">is_default</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">build</span><span class="p">)</span> <span class="ow">and</span> <span class="n">might_have_unbuilt_state</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
            <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;`build()` was called on layer &#39;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="si">}</span><span class="s2">&#39;, however &quot;</span>
                <span class="s2">&quot;the layer does not have a `build()` method implemented &quot;</span>
                <span class="s2">&quot;and it looks like it has unbuilt state. This will cause &quot;</span>
                <span class="s2">&quot;the layer to be marked as built, despite not being &quot;</span>
                <span class="s2">&quot;actually built, which may cause failures down the line. &quot;</span>
                <span class="s2">&quot;Make sure to implement a proper `build()` method.&quot;</span>
            <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">built</span> <span class="o">=</span> <span class="kc">True</span>

    <span class="k">def</span> <span class="nf">_lock_state</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Prevent further state updates, called automatically in `build()`.&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">_tracker</span><span class="o">.</span><span class="n">locked</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_tracker</span><span class="o">.</span><span class="n">lock</span><span class="p">(</span>
                <span class="n">msg</span><span class="o">=</span><span class="p">(</span>
                    <span class="s2">&quot;You cannot add new elements of state &quot;</span>
                    <span class="s2">&quot;(variables or sub-layers) &quot;</span>
                    <span class="s2">&quot;to a layer that is already built. All state &quot;</span>
                    <span class="s2">&quot;must be created in the `__init__()` method or &quot;</span>
                    <span class="s2">&quot;in the `build()` method.&quot;</span>
                <span class="p">)</span>
            <span class="p">)</span>

    <span class="k">def</span> <span class="nf">get_build_config</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Returns a dictionary with the layer&#39;s input shape.</span>

<span class="sd">        This method returns a config dict that can be used by</span>
<span class="sd">        `build_from_config(config)` to create all states (e.g. Variables and</span>
<span class="sd">        Lookup tables) needed by the layer.</span>

<span class="sd">        By default, the config only contains the input shape that the layer</span>
<span class="sd">        was built with. If you&#39;re writing a custom layer that creates state in</span>
<span class="sd">        an unusual way, you should override this method to make sure this state</span>
<span class="sd">        is already created when Keras attempts to load its value upon model</span>
<span class="sd">        loading.</span>

<span class="sd">        Returns:</span>
<span class="sd">            A dict containing the input shape associated with the layer.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_build_shapes_dict</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_build_shapes_dict</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
                <span class="k">return</span> <span class="p">{</span>
                    <span class="s2">&quot;input_shape&quot;</span><span class="p">:</span> <span class="nb">tuple</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_build_shapes_dict</span><span class="o">.</span><span class="n">values</span><span class="p">())[</span><span class="mi">0</span><span class="p">],</span>
                <span class="p">}</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">return</span> <span class="p">{</span><span class="s2">&quot;shapes_dict&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">_build_shapes_dict</span><span class="p">}</span>

    <span class="k">def</span> <span class="nf">build_from_config</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Builds the layer&#39;s states with the supplied config dict.</span>

<span class="sd">        By default, this method calls the `build(config[&quot;input_shape&quot;])` method,</span>
<span class="sd">        which creates weights based on the layer&#39;s input shape in the supplied</span>
<span class="sd">        config. If your config contains other information needed to load the</span>
<span class="sd">        layer&#39;s state, you should override this method.</span>

<span class="sd">        Args:</span>
<span class="sd">            config: Dict containing the input shape associated with this layer.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">config</span><span class="p">:</span>
            <span class="k">if</span> <span class="s2">&quot;input_shape&quot;</span> <span class="ow">in</span> <span class="n">config</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">build</span><span class="p">(</span><span class="n">config</span><span class="p">[</span><span class="s2">&quot;input_shape&quot;</span><span class="p">])</span>
            <span class="k">elif</span> <span class="s2">&quot;shapes_dict&quot;</span> <span class="ow">in</span> <span class="n">config</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">build</span><span class="p">(</span><span class="o">**</span><span class="n">config</span><span class="p">[</span><span class="s2">&quot;shapes_dict&quot;</span><span class="p">])</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">built</span> <span class="o">=</span> <span class="kc">True</span>

    <span class="k">def</span> <span class="nf">_obj_type</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="s2">&quot;Layer&quot;</span>

    <span class="k">def</span> <span class="nf">add_variable</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">shape</span><span class="p">,</span>
        <span class="n">initializer</span><span class="p">,</span>
        <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">trainable</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">autocast</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">regularizer</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">constraint</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Add a weight variable to the layer.</span>

<span class="sd">        Alias of `add_weight()`.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">add_weight</span><span class="p">(</span>
            <span class="n">shape</span><span class="o">=</span><span class="n">shape</span><span class="p">,</span>
            <span class="n">initializer</span><span class="o">=</span><span class="n">initializer</span><span class="p">,</span>
            <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span>
            <span class="n">trainable</span><span class="o">=</span><span class="n">trainable</span><span class="p">,</span>
            <span class="n">autocast</span><span class="o">=</span><span class="n">autocast</span><span class="p">,</span>
            <span class="n">regularizer</span><span class="o">=</span><span class="n">regularizer</span><span class="p">,</span>
            <span class="n">constraint</span><span class="o">=</span><span class="n">constraint</span><span class="p">,</span>
            <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">add_weight</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">shape</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">initializer</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">trainable</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">autocast</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">regularizer</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">constraint</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">aggregation</span><span class="o">=</span><span class="s2">&quot;mean&quot;</span><span class="p">,</span>
        <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Add a weight variable to the layer.</span>

<span class="sd">        Args:</span>
<span class="sd">            shape: Shape tuple for the variable. Must be fully-defined</span>
<span class="sd">                (no `None` entries). Defaults to `()` (scalar) if unspecified.</span>
<span class="sd">            initializer: Initializer object to use to populate the initial</span>
<span class="sd">                variable value, or string name of a built-in initializer</span>
<span class="sd">                (e.g. `&quot;random_normal&quot;`). If unspecified, defaults to</span>
<span class="sd">                `&quot;glorot_uniform&quot;` for floating-point variables and to `&quot;zeros&quot;`</span>
<span class="sd">                for all other types (e.g. int, bool).</span>
<span class="sd">            dtype: Dtype of the variable to create, e.g. `&quot;float32&quot;`. If</span>
<span class="sd">                unspecified, defaults to the layer&#39;s variable dtype</span>
<span class="sd">                (which itself defaults to `&quot;float32&quot;` if unspecified).</span>
<span class="sd">            trainable: Boolean, whether the variable should be trainable via</span>
<span class="sd">                backprop or whether its updates are managed manually. Defaults</span>
<span class="sd">                to `True`.</span>
<span class="sd">            autocast: Boolean, whether to autocast layers variables when</span>
<span class="sd">                accessing them. Defaults to `True`.</span>
<span class="sd">            regularizer: Regularizer object to call to apply penalty on the</span>
<span class="sd">                weight. These penalties are summed into the loss function</span>
<span class="sd">                during optimization. Defaults to `None`.</span>
<span class="sd">            constraint: Contrainst object to call on the variable after any</span>
<span class="sd">                optimizer update, or string name of a built-in constraint.</span>
<span class="sd">                Defaults to `None`.</span>
<span class="sd">            aggregation: String, one of `&#39;mean&#39;`, `&#39;sum&#39;`,</span>
<span class="sd">                `&#39;only_first_replica&#39;`. Annotates the variable with the type</span>
<span class="sd">                of multi-replica aggregation to be used for this variable</span>
<span class="sd">                when writing custom data parallel training loops.</span>
<span class="sd">            name: String name of the variable. Useful for debugging purposes.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_check_super_called</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">shape</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">shape</span> <span class="o">=</span> <span class="p">()</span>
        <span class="k">if</span> <span class="n">dtype</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">dtype</span> <span class="o">=</span> <span class="n">backend</span><span class="o">.</span><span class="n">standardize_dtype</span><span class="p">(</span><span class="n">dtype</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">dtype</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">variable_dtype</span>
        <span class="k">if</span> <span class="n">initializer</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="s2">&quot;float&quot;</span> <span class="ow">in</span> <span class="n">dtype</span><span class="p">:</span>
                <span class="n">initializer</span> <span class="o">=</span> <span class="s2">&quot;glorot_uniform&quot;</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">initializer</span> <span class="o">=</span> <span class="s2">&quot;zeros&quot;</span>
        <span class="n">initializer</span> <span class="o">=</span> <span class="n">initializers</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">initializer</span><span class="p">)</span>
        <span class="k">with</span> <span class="n">backend</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">caller</span><span class="o">=</span><span class="bp">self</span><span class="p">):</span>
            <span class="n">variable</span> <span class="o">=</span> <span class="n">backend</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span>
                <span class="n">initializer</span><span class="o">=</span><span class="n">initializer</span><span class="p">,</span>
                <span class="n">shape</span><span class="o">=</span><span class="n">shape</span><span class="p">,</span>
                <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span>
                <span class="n">trainable</span><span class="o">=</span><span class="n">trainable</span><span class="p">,</span>
                <span class="n">autocast</span><span class="o">=</span><span class="n">autocast</span><span class="p">,</span>
                <span class="n">aggregation</span><span class="o">=</span><span class="n">aggregation</span><span class="p">,</span>
                <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="c1"># Will be added to layer.losses</span>
        <span class="n">variable</span><span class="o">.</span><span class="n">regularizer</span> <span class="o">=</span> <span class="n">regularizers</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">regularizer</span><span class="p">)</span>
        <span class="n">variable</span><span class="o">.</span><span class="n">constraint</span> <span class="o">=</span> <span class="n">constraints</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">constraint</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_track_variable</span><span class="p">(</span><span class="n">variable</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">variable</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">trainable</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Settable boolean, whether this layer should be trainable or not.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_trainable</span>

    <span class="nd">@trainable</span><span class="o">.</span><span class="n">setter</span>
    <span class="k">def</span> <span class="nf">trainable</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Sets trainable attribute for the layer and its sublayers.</span>

<span class="sd">        When this value is changed during training (e.g. with a</span>
<span class="sd">        `Callback`) you need to call the parent</span>
<span class="sd">        `Model.make_train_function` with `force=True` in order to</span>
<span class="sd">        recompile the training graph.</span>

<span class="sd">        Args:</span>
<span class="sd">            value: Boolean with the desired state for the layer&#39;s trainable</span>
<span class="sd">                attribute.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">value</span> <span class="o">=</span> <span class="nb">bool</span><span class="p">(</span><span class="n">value</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_trainable</span> <span class="o">=</span> <span class="n">value</span>
        <span class="k">for</span> <span class="n">v</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_trainable_variables</span><span class="p">:</span>
            <span class="n">v</span><span class="o">.</span><span class="n">trainable</span> <span class="o">=</span> <span class="n">value</span>
        <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_layers</span><span class="p">:</span>
            <span class="n">layer</span><span class="o">.</span><span class="n">trainable</span> <span class="o">=</span> <span class="n">value</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">variables</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;List of all layer state, including random seeds.</span>

<span class="sd">        This extends `layer.weights` to include all state used by the layer</span>
<span class="sd">        including `SeedGenerator`s.</span>

<span class="sd">        Note that metrics variables are not included here, use</span>
<span class="sd">        `metrics_variables` to visit all the metric variables.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Return all `Variables` associate with the layer including metrics</span>
        <span class="c1"># and random seeds. Also deduplicate them.</span>
        <span class="n">variables</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">seen_ids</span> <span class="o">=</span> <span class="nb">set</span><span class="p">()</span>
        <span class="k">for</span> <span class="n">v</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_trainable_variables</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">_non_trainable_variables</span><span class="p">:</span>
            <span class="k">if</span> <span class="nb">id</span><span class="p">(</span><span class="n">v</span><span class="p">)</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">seen_ids</span><span class="p">:</span>
                <span class="n">variables</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">v</span><span class="p">)</span>
                <span class="n">seen_ids</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="nb">id</span><span class="p">(</span><span class="n">v</span><span class="p">))</span>
        <span class="k">for</span> <span class="n">sg</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_seed_generators</span><span class="p">:</span>
            <span class="n">variables</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">sg</span><span class="o">.</span><span class="n">state</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_layers</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">layer</span><span class="o">.</span><span class="n">variables</span><span class="p">:</span>
                <span class="k">if</span> <span class="nb">id</span><span class="p">(</span><span class="n">v</span><span class="p">)</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">seen_ids</span><span class="p">:</span>
                    <span class="n">variables</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">v</span><span class="p">)</span>
                    <span class="n">seen_ids</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="nb">id</span><span class="p">(</span><span class="n">v</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">variables</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">trainable_variables</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;List of all trainable layer state.</span>

<span class="sd">        This is equivalent to `layer.trainable_weights`.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">trainable</span><span class="p">:</span>
            <span class="k">return</span> <span class="p">[]</span>
        <span class="k">return</span> <span class="p">[</span><span class="n">v</span> <span class="k">for</span> <span class="n">v</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">variables</span> <span class="k">if</span> <span class="n">v</span><span class="o">.</span><span class="n">trainable</span><span class="p">]</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">non_trainable_variables</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;List of all non-trainable layer state.</span>

<span class="sd">        This extends `layer.non_trainable_weights` to include all state used by</span>
<span class="sd">        the layer including state for metrics and `SeedGenerator`s.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">trainable</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">variables</span>
        <span class="k">return</span> <span class="p">[</span><span class="n">v</span> <span class="k">for</span> <span class="n">v</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">variables</span> <span class="k">if</span> <span class="ow">not</span> <span class="n">v</span><span class="o">.</span><span class="n">trainable</span><span class="p">]</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">weights</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;List of all weight variables of the layer.</span>

<span class="sd">        Unlike, `layer.variables` this excludes metric state and random seeds.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Return only `Variables` directly owned by layers and sub-layers.</span>
        <span class="c1"># Also deduplicate them.</span>
        <span class="n">weights</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">seen_ids</span> <span class="o">=</span> <span class="nb">set</span><span class="p">()</span>
        <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_trainable_variables</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">_non_trainable_variables</span><span class="p">:</span>
            <span class="k">if</span> <span class="nb">id</span><span class="p">(</span><span class="n">w</span><span class="p">)</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">seen_ids</span><span class="p">:</span>
                <span class="n">weights</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">w</span><span class="p">)</span>
                <span class="n">seen_ids</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="nb">id</span><span class="p">(</span><span class="n">w</span><span class="p">))</span>
        <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_layers</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">layer</span><span class="o">.</span><span class="n">weights</span><span class="p">:</span>
                <span class="k">if</span> <span class="nb">id</span><span class="p">(</span><span class="n">w</span><span class="p">)</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">seen_ids</span><span class="p">:</span>
                    <span class="n">weights</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">w</span><span class="p">)</span>
                    <span class="n">seen_ids</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="nb">id</span><span class="p">(</span><span class="n">w</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">weights</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">trainable_weights</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;List of all trainable weight variables of the layer.</span>

<span class="sd">        These are the weights that get updated by the optimizer during training.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">trainable</span><span class="p">:</span>
            <span class="k">return</span> <span class="p">[]</span>
        <span class="k">return</span> <span class="p">[</span><span class="n">v</span> <span class="k">for</span> <span class="n">v</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">weights</span> <span class="k">if</span> <span class="n">v</span><span class="o">.</span><span class="n">trainable</span><span class="p">]</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">non_trainable_weights</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;List of all non-trainable weight variables of the layer.</span>

<span class="sd">        These are the weights that should not be updated by the optimizer during</span>
<span class="sd">        training. Unlike, `layer.non_trainable_variables` this excludes metric</span>
<span class="sd">        state and random seeds.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">trainable</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">weights</span>
        <span class="k">return</span> <span class="p">[</span><span class="n">v</span> <span class="k">for</span> <span class="n">v</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">weights</span> <span class="k">if</span> <span class="ow">not</span> <span class="n">v</span><span class="o">.</span><span class="n">trainable</span><span class="p">]</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">metrics</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;List of all metrics.&quot;&quot;&quot;</span>
        <span class="n">metrics</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_metrics</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_layers</span><span class="p">:</span>
            <span class="n">metrics</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">layer</span><span class="o">.</span><span class="n">metrics</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">metrics</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">metrics_variables</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;List of all metric variables.&quot;&quot;&quot;</span>
        <span class="nb">vars</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">metric</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">metrics</span><span class="p">:</span>
            <span class="nb">vars</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">metric</span><span class="o">.</span><span class="n">variables</span><span class="p">)</span>
        <span class="k">return</span> <span class="nb">vars</span>

    <span class="k">def</span> <span class="nf">get_weights</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Return the values of `layer.weights` as a list of NumPy arrays.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="p">[</span><span class="n">v</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span> <span class="k">for</span> <span class="n">v</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="p">]</span>

    <span class="k">def</span> <span class="nf">set_weights</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">weights</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Sets the values of `layer.weights` from a list of NumPy arrays.&quot;&quot;&quot;</span>
        <span class="n">layer_weights</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">weights</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">layer_weights</span><span class="p">)</span> <span class="o">!=</span> <span class="nb">len</span><span class="p">(</span><span class="n">weights</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;You called `set_weights(weights)` on layer &#39;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="si">}</span><span class="s2">&#39; &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;with a weight list of length </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">weights</span><span class="p">)</span><span class="si">}</span><span class="s2">, but the layer &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;was expecting </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">layer_weights</span><span class="p">)</span><span class="si">}</span><span class="s2"> weights.&quot;</span>
            <span class="p">)</span>
        <span class="k">for</span> <span class="n">variable</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">layer_weights</span><span class="p">,</span> <span class="n">weights</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">variable</span><span class="o">.</span><span class="n">shape</span> <span class="o">!=</span> <span class="n">value</span><span class="o">.</span><span class="n">shape</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;Layer </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="si">}</span><span class="s2"> weight shape </span><span class="si">{</span><span class="n">variable</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2"> &quot;</span>
                    <span class="s2">&quot;is not compatible with provided weight &quot;</span>
                    <span class="sa">f</span><span class="s2">&quot;shape </span><span class="si">{</span><span class="n">value</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">.&quot;</span>
                <span class="p">)</span>
            <span class="n">variable</span><span class="o">.</span><span class="n">assign</span><span class="p">(</span><span class="n">value</span><span class="p">)</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">dtype_policy</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_dtype_policy</span>

    <span class="nd">@dtype_policy</span><span class="o">.</span><span class="n">setter</span>
    <span class="k">def</span> <span class="nf">dtype_policy</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="n">policy</span> <span class="o">=</span> <span class="n">dtype_policies</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">value</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_dtype_policy</span><span class="p">,</span> <span class="n">DTypePolicyMap</span><span class="p">)</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">path</span><span class="p">:</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">path</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_dtype_policy</span><span class="p">:</span>
                <span class="k">del</span> <span class="bp">self</span><span class="o">.</span><span class="n">_dtype_policy</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">path</span><span class="p">]</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_dtype_policy</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">path</span><span class="p">]</span> <span class="o">=</span> <span class="n">policy</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_dtype_policy</span> <span class="o">=</span> <span class="n">policy</span>
        <span class="k">if</span> <span class="n">policy</span><span class="o">.</span><span class="n">quantization_mode</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">built</span> <span class="ow">and</span> <span class="ow">not</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;_is_quantized&quot;</span><span class="p">,</span> <span class="kc">False</span><span class="p">):</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">quantize</span><span class="p">(</span><span class="n">policy</span><span class="o">.</span><span class="n">quantization_mode</span><span class="p">)</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Alias of `layer.variable_dtype`.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">variable_dtype</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">compute_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;The dtype of the computations performed by the layer.&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_dtype_policy</span><span class="p">,</span> <span class="n">DTypePolicyMap</span><span class="p">)</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">path</span><span class="p">:</span>
            <span class="n">policy</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_dtype_policy</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">path</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">policy</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_dtype_policy</span>
        <span class="k">return</span> <span class="n">policy</span><span class="o">.</span><span class="n">compute_dtype</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">variable_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;The dtype of the state (weights) of the layer.&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_dtype_policy</span><span class="p">,</span> <span class="n">DTypePolicyMap</span><span class="p">)</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">path</span><span class="p">:</span>
            <span class="n">policy</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_dtype_policy</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">path</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">policy</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_dtype_policy</span>
        <span class="k">return</span> <span class="n">policy</span><span class="o">.</span><span class="n">variable_dtype</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">quantization_mode</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;The quantization mode of this layer, `None` if not quantized.&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_dtype_policy</span><span class="p">,</span> <span class="n">DTypePolicyMap</span><span class="p">)</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">path</span><span class="p">:</span>
            <span class="n">policy</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_dtype_policy</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">path</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">policy</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_dtype_policy</span>
        <span class="k">return</span> <span class="n">policy</span><span class="o">.</span><span class="n">quantization_mode</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">input_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;The dtype layer inputs should be converted to.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">compute_dtype</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">supports_masking</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Whether this layer supports computing a mask using `compute_mask`.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_supports_masking</span>

    <span class="nd">@supports_masking</span><span class="o">.</span><span class="n">setter</span>
    <span class="k">def</span> <span class="nf">supports_masking</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_supports_masking</span> <span class="o">=</span> <span class="n">value</span>

    <span class="nd">@utils</span><span class="o">.</span><span class="n">default</span>
    <span class="k">def</span> <span class="nf">compute_mask</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">previous_mask</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">previous_mask</span>

    <span class="nd">@traceback_utils</span><span class="o">.</span><span class="n">filter_traceback</span>
    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_check_super_called</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_called</span> <span class="o">=</span> <span class="kc">True</span>

        <span class="c1">#####################################</span>
        <span class="c1"># 1. Convert any array arguments to tensors of correct dtype.</span>
        <span class="k">def</span> <span class="nf">maybe_convert</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">dtype_policy</span><span class="o">.</span><span class="n">convert_input</span><span class="p">(</span>
                <span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">autocast</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">input_dtype</span>
            <span class="p">)</span>

        <span class="c1"># Used to avoid expensive `tree` operations in the most common case.</span>
        <span class="k">if</span> <span class="p">(</span>
            <span class="n">kwargs</span>
            <span class="ow">or</span> <span class="nb">len</span><span class="p">(</span><span class="n">args</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">1</span>
            <span class="ow">or</span> <span class="ow">not</span> <span class="n">backend</span><span class="o">.</span><span class="n">is_tensor</span><span class="p">(</span><span class="n">args</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
            <span class="ow">or</span> <span class="n">backend</span><span class="o">.</span><span class="n">standardize_dtype</span><span class="p">(</span><span class="n">args</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span> <span class="o">!=</span> <span class="bp">self</span><span class="o">.</span><span class="n">input_dtype</span>
        <span class="p">)</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">_convert_input_args</span><span class="p">:</span>
            <span class="n">args</span> <span class="o">=</span> <span class="n">tree</span><span class="o">.</span><span class="n">map_structure</span><span class="p">(</span><span class="n">maybe_convert</span><span class="p">,</span> <span class="n">args</span><span class="p">)</span>
            <span class="n">kwargs</span> <span class="o">=</span> <span class="n">tree</span><span class="o">.</span><span class="n">map_structure</span><span class="p">(</span><span class="n">maybe_convert</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">)</span>

        <span class="c1">##########################################################</span>
        <span class="c1"># 2. Enforce that only tensors can be passed positionally.</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">_allow_non_tensor_positional_args</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">arg</span> <span class="ow">in</span> <span class="n">tree</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="n">args</span><span class="p">):</span>
                <span class="k">if</span> <span class="p">(</span>
                    <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">arg</span><span class="p">,</span> <span class="n">KerasTensor</span><span class="p">)</span>
                    <span class="ow">and</span> <span class="ow">not</span> <span class="n">backend</span><span class="o">.</span><span class="n">is_tensor</span><span class="p">(</span><span class="n">arg</span><span class="p">)</span>
                    <span class="ow">and</span> <span class="n">arg</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
                <span class="p">):</span>
                    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                        <span class="s2">&quot;Only input tensors may be passed as &quot;</span>
                        <span class="s2">&quot;positional arguments. The following argument value &quot;</span>
                        <span class="sa">f</span><span class="s2">&quot;should be passed as a keyword argument: </span><span class="si">{</span><span class="n">arg</span><span class="si">}</span><span class="s2"> &quot;</span>
                        <span class="sa">f</span><span class="s2">&quot;(of type </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">arg</span><span class="p">)</span><span class="si">}</span><span class="s2">)&quot;</span>
                    <span class="p">)</span>

        <span class="c1"># Caches info about `call()` signature, args, kwargs.</span>
        <span class="n">call_spec</span> <span class="o">=</span> <span class="n">CallSpec</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_call_signature</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">)</span>

        <span class="c1">############################################</span>
        <span class="c1"># 3. Check input spec for 1st positional arg.</span>
        <span class="c1"># TODO: consider extending this to all args and kwargs.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_assert_input_compatibility</span><span class="p">(</span><span class="n">call_spec</span><span class="o">.</span><span class="n">first_arg</span><span class="p">)</span>

        <span class="c1">################</span>
        <span class="c1"># 4. Call build</span>
        <span class="k">with</span> <span class="bp">self</span><span class="o">.</span><span class="n">_open_name_scope</span><span class="p">():</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_maybe_build</span><span class="p">(</span><span class="n">call_spec</span><span class="p">)</span>

        <span class="c1">##########################</span>
        <span class="c1"># 5. Infer training value</span>
        <span class="c1"># Training phase for `Layer.call` is set via (in order of priority):</span>
        <span class="c1"># (1) The `training` argument passed to this `Layer.call`, if not None</span>
        <span class="c1"># (2) The training argument of an outer `Layer.call`.</span>
        <span class="c1"># (4) Any non-None default value for `training` in the call signature</span>
        <span class="c1"># (5) False (treating the layer as if it&#39;s in inference)</span>

        <span class="c1"># Maintains info about the `Layer.call` stack</span>
        <span class="c1"># across nested calls.</span>
        <span class="n">call_context</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_call_context</span><span class="p">()</span>

        <span class="c1"># This is the value explicitly passed by the user</span>
        <span class="n">training</span> <span class="o">=</span> <span class="n">call_spec</span><span class="o">.</span><span class="n">user_arguments_dict</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;training&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">training</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="c1"># Wasn&#39;t passed explicitly: use context value</span>
            <span class="n">training</span> <span class="o">=</span> <span class="n">call_context</span><span class="o">.</span><span class="n">training</span>
            <span class="k">if</span> <span class="n">training</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="c1"># Get signature default value</span>
                <span class="n">training</span> <span class="o">=</span> <span class="n">call_spec</span><span class="o">.</span><span class="n">arguments_dict</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;training&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="n">call_context</span><span class="o">.</span><span class="n">training</span> <span class="o">=</span> <span class="n">training</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_call_has_training_arg</span> <span class="ow">and</span> <span class="n">training</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="c1"># Only populate arg if it has a concrete value</span>
            <span class="n">kwargs</span><span class="p">[</span><span class="s2">&quot;training&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">training</span>

        <span class="c1">##############################</span>
        <span class="c1"># 6. Populate mask argument(s)</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">call_spec</span><span class="o">.</span><span class="n">tensor_arguments_dict</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="k">if</span> <span class="p">(</span>
                <span class="s2">&quot;mask&quot;</span> <span class="ow">in</span> <span class="n">call_spec</span><span class="o">.</span><span class="n">argument_names</span>
                <span class="ow">and</span> <span class="n">call_spec</span><span class="o">.</span><span class="n">arguments_dict</span><span class="p">[</span><span class="s2">&quot;mask&quot;</span><span class="p">]</span> <span class="ow">is</span> <span class="kc">None</span>
            <span class="p">):</span>
                <span class="n">arg_name</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">call_spec</span><span class="o">.</span><span class="n">tensor_arguments_dict</span><span class="o">.</span><span class="n">keys</span><span class="p">())[</span><span class="mi">0</span><span class="p">]</span>
                <span class="n">only_tensor_arg</span> <span class="o">=</span> <span class="n">call_spec</span><span class="o">.</span><span class="n">tensor_arguments_dict</span><span class="p">[</span><span class="n">arg_name</span><span class="p">]</span>
                <span class="n">mask</span> <span class="o">=</span> <span class="n">tree</span><span class="o">.</span><span class="n">map_structure</span><span class="p">(</span>
                    <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="s2">&quot;_keras_mask&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">),</span>
                    <span class="n">only_tensor_arg</span><span class="p">,</span>
                <span class="p">)</span>
                <span class="n">kwargs</span><span class="p">[</span><span class="s2">&quot;mask&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">mask</span>
        <span class="k">elif</span> <span class="nb">len</span><span class="p">(</span><span class="n">call_spec</span><span class="o">.</span><span class="n">tensor_arguments_dict</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">call_spec</span><span class="o">.</span><span class="n">tensor_arguments_dict</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
                <span class="n">expected_mask_arg_name</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">k</span><span class="si">}</span><span class="s2">_mask&quot;</span>
                <span class="k">if</span> <span class="n">expected_mask_arg_name</span> <span class="ow">in</span> <span class="n">call_spec</span><span class="o">.</span><span class="n">argument_names</span><span class="p">:</span>
                    <span class="k">if</span> <span class="n">call_spec</span><span class="o">.</span><span class="n">arguments_dict</span><span class="p">[</span><span class="n">expected_mask_arg_name</span><span class="p">]</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                        <span class="n">mask</span> <span class="o">=</span> <span class="n">tree</span><span class="o">.</span><span class="n">map_structure</span><span class="p">(</span>
                            <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="s2">&quot;_keras_mask&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">),</span> <span class="n">v</span>
                        <span class="p">)</span>
                        <span class="n">kwargs</span><span class="p">[</span><span class="n">expected_mask_arg_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">mask</span>

        <span class="c1">####################</span>
        <span class="c1"># 7. Call the layer.</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="k">with</span> <span class="bp">self</span><span class="o">.</span><span class="n">_open_name_scope</span><span class="p">():</span>
                <span class="n">current_scope</span> <span class="o">=</span> <span class="n">backend</span><span class="o">.</span><span class="n">get_autocast_scope</span><span class="p">()</span>
                <span class="n">new_scope</span> <span class="o">=</span> <span class="kc">None</span>
                <span class="k">if</span> <span class="n">current_scope</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                    <span class="c1"># Clear or update the current scope if necessary.</span>
                    <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">autocast</span><span class="p">:</span>
                        <span class="n">new_scope</span> <span class="o">=</span> <span class="n">backend</span><span class="o">.</span><span class="n">AutocastScope</span><span class="p">(</span><span class="kc">None</span><span class="p">)</span>
                    <span class="k">elif</span> <span class="ow">not</span> <span class="n">backend</span><span class="o">.</span><span class="n">is_float_dtype</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">compute_dtype</span><span class="p">):</span>
                        <span class="c1"># Some preprocessing layers might have a non-float</span>
                        <span class="c1"># dtype, we should not autocast in this case.</span>
                        <span class="n">new_scope</span> <span class="o">=</span> <span class="n">backend</span><span class="o">.</span><span class="n">AutocastScope</span><span class="p">(</span><span class="kc">None</span><span class="p">)</span>
                    <span class="k">elif</span> <span class="n">current_scope</span><span class="o">.</span><span class="n">dtype</span> <span class="o">!=</span> <span class="bp">self</span><span class="o">.</span><span class="n">compute_dtype</span><span class="p">:</span>
                        <span class="n">new_scope</span> <span class="o">=</span> <span class="n">backend</span><span class="o">.</span><span class="n">AutocastScope</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">compute_dtype</span><span class="p">)</span>
                <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">compute_dtype</span> <span class="o">!=</span> <span class="bp">self</span><span class="o">.</span><span class="n">variable_dtype</span><span class="p">:</span>
                    <span class="c1"># Enter a new scope if our dtypes are &quot;mixed&quot;.</span>
                    <span class="n">new_scope</span> <span class="o">=</span> <span class="n">backend</span><span class="o">.</span><span class="n">AutocastScope</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">compute_dtype</span><span class="p">)</span>

                <span class="k">if</span> <span class="n">new_scope</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                    <span class="k">with</span> <span class="n">new_scope</span><span class="p">:</span>
                        <span class="n">outputs</span> <span class="o">=</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">outputs</span> <span class="o">=</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
                <span class="c1"># Change the layout for the layer output if needed.</span>
                <span class="c1"># This is useful for relayout intermediate tensor in the model</span>
                <span class="c1"># to achieve the optimal performance.</span>
                <span class="n">distribution</span> <span class="o">=</span> <span class="n">distribution_lib</span><span class="o">.</span><span class="n">distribution</span><span class="p">()</span>
                <span class="k">if</span> <span class="n">distribution</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                    <span class="n">current_layer_path</span> <span class="o">=</span> <span class="n">current_path</span><span class="p">()</span>
                    <span class="n">current_layer_path</span> <span class="o">+=</span> <span class="s2">&quot;/output&quot;</span>
                    <span class="n">layout</span> <span class="o">=</span> <span class="n">distribution</span><span class="o">.</span><span class="n">get_tensor_layout</span><span class="p">(</span><span class="n">current_layer_path</span><span class="p">)</span>
                    <span class="k">if</span> <span class="n">layout</span><span class="p">:</span>
                        <span class="n">outputs</span> <span class="o">=</span> <span class="n">distribution_lib</span><span class="o">.</span><span class="n">distribute_tensor</span><span class="p">(</span>
                            <span class="n">outputs</span><span class="p">,</span> <span class="n">layout</span>
                        <span class="p">)</span>

                <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">built</span><span class="p">:</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">built</span> <span class="o">=</span> <span class="kc">True</span>
                <span class="c1"># Record activity regularizer loss.</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">activity_regularizer</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                    <span class="k">for</span> <span class="n">output</span> <span class="ow">in</span> <span class="n">tree</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="n">outputs</span><span class="p">):</span>
                        <span class="k">if</span> <span class="n">backend</span><span class="o">.</span><span class="n">is_tensor</span><span class="p">(</span><span class="n">output</span><span class="p">):</span>
                            <span class="bp">self</span><span class="o">.</span><span class="n">add_loss</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">activity_regularizer</span><span class="p">(</span><span class="n">output</span><span class="p">))</span>

            <span class="c1"># Set masks on outputs,</span>
            <span class="c1"># provided only the first positional input arg and its mask.</span>
            <span class="c1"># TODO: consider extending this to all args and kwargs.</span>
            <span class="n">previous_mask</span> <span class="o">=</span> <span class="n">tree</span><span class="o">.</span><span class="n">map_structure</span><span class="p">(</span>
                <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="s2">&quot;_keras_mask&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">),</span> <span class="n">call_spec</span><span class="o">.</span><span class="n">first_arg</span>
            <span class="p">)</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">supports_masking</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_set_mask_metadata</span><span class="p">(</span>
                    <span class="n">call_spec</span><span class="o">.</span><span class="n">first_arg</span><span class="p">,</span> <span class="n">outputs</span><span class="p">,</span> <span class="n">previous_mask</span>
                <span class="p">)</span>
            <span class="k">elif</span> <span class="nb">any</span><span class="p">(</span><span class="n">m</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="n">tree</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="n">previous_mask</span><span class="p">)):</span>
                <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;Layer &#39;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="si">}</span><span class="s2">&#39; (of type </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2">) &quot;</span>
                    <span class="s2">&quot;was passed an input with a mask attached to it. &quot;</span>
                    <span class="s2">&quot;However, this layer does not support masking and will &quot;</span>
                    <span class="s2">&quot;therefore destroy the mask information. Downstream &quot;</span>
                    <span class="s2">&quot;layers will not see the mask.&quot;</span>
                <span class="p">)</span>
        <span class="k">finally</span><span class="p">:</span>
            <span class="c1"># Destroy call context if we created it</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_maybe_reset_call_context</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">outputs</span>

    <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="k">raise</span> <span class="bp">self</span><span class="o">.</span><span class="n">_not_implemented_error</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">call</span><span class="p">)</span>

    <span class="nd">@traceback_utils</span><span class="o">.</span><span class="n">filter_traceback</span>
    <span class="k">def</span> <span class="nf">stateless_call</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">trainable_variables</span><span class="p">,</span>
        <span class="n">non_trainable_variables</span><span class="p">,</span>
        <span class="o">*</span><span class="n">args</span><span class="p">,</span>
        <span class="n">return_losses</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Call the layer without any side effects.</span>

<span class="sd">        Args:</span>
<span class="sd">            trainable_variables: List of trainable variables of the model.</span>
<span class="sd">            non_trainable_variables: List of non-trainable variables of the</span>
<span class="sd">                model.</span>
<span class="sd">            *args: Positional arguments to be passed to `call()`.</span>
<span class="sd">            return_losses: If `True`, `stateless_call()` will return the list of</span>
<span class="sd">                losses created during `call()` as part of its return values.</span>
<span class="sd">            **kwargs: Keyword arguments to be passed to `call()`.</span>

<span class="sd">        Returns:</span>
<span class="sd">            A tuple. By default, returns `(outputs, non_trainable_variables)`.</span>
<span class="sd">                If `return_losses = True`, then returns</span>
<span class="sd">                `(outputs, non_trainable_variables, losses)`.</span>

<span class="sd">        Note: `non_trainable_variables` include not only non-trainable weights</span>
<span class="sd">        such as `BatchNormalization` statistics, but also RNG seed state</span>
<span class="sd">        (if there are any random operations part of the layer, such as dropout),</span>
<span class="sd">        and `Metric` state (if there are any metrics attached to the layer).</span>
<span class="sd">        These are all elements of state of the layer.</span>

<span class="sd">        Example:</span>

<span class="sd">        ```python</span>
<span class="sd">        model = ...</span>
<span class="sd">        data = ...</span>
<span class="sd">        trainable_variables = model.trainable_variables</span>
<span class="sd">        non_trainable_variables = model.non_trainable_variables</span>
<span class="sd">        # Call the model with zero side effects</span>
<span class="sd">        outputs, non_trainable_variables = model.stateless_call(</span>
<span class="sd">            trainable_variables,</span>
<span class="sd">            non_trainable_variables,</span>
<span class="sd">            data,</span>
<span class="sd">        )</span>
<span class="sd">        # Attach the updated state to the model</span>
<span class="sd">        # (until you do this, the model is still in its pre-call state).</span>
<span class="sd">        for ref_var, value in zip(</span>
<span class="sd">            model.non_trainable_variables, non_trainable_variables</span>
<span class="sd">        ):</span>
<span class="sd">            ref_var.assign(value)</span>
<span class="sd">        ```</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_check_super_called</span><span class="p">()</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">built</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;To call stateless_call, </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2"> must be &quot;</span>
                <span class="s2">&quot;built (i.e. its variables must have been already created). &quot;</span>
                <span class="s2">&quot;You can build it by calling it on some data.&quot;</span>
            <span class="p">)</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">trainable_variables</span><span class="p">)</span> <span class="o">!=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">trainable_variables</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;Argument `trainable_variables` must be a list of tensors &quot;</span>
                <span class="s2">&quot;corresponding 1:1 to &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2">().trainable_variables. &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;Received list with length </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">trainable_variables</span><span class="p">)</span><span class="si">}</span><span class="s2">, &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;but expected </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">trainable_variables</span><span class="p">)</span><span class="si">}</span><span class="s2"> variables.&quot;</span>
            <span class="p">)</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">non_trainable_variables</span><span class="p">)</span> <span class="o">!=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">non_trainable_variables</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;Argument `non_trainable_variables` must be a list of tensors &quot;</span>
                <span class="s2">&quot;corresponding 1:1 to &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2">().non_trainable_variables. &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;Received list with length </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">non_trainable_variables</span><span class="p">)</span><span class="si">}</span><span class="s2">, &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;but expected </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">non_trainable_variables</span><span class="p">)</span><span class="si">}</span><span class="s2"> variables.&quot;</span>
            <span class="p">)</span>

        <span class="c1"># Gather variable mapping</span>
        <span class="n">trainable_mapping</span> <span class="o">=</span> <span class="nb">zip</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">trainable_variables</span><span class="p">,</span> <span class="n">trainable_variables</span><span class="p">)</span>
        <span class="n">non_trainable_mapping</span> <span class="o">=</span> <span class="nb">zip</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">non_trainable_variables</span><span class="p">,</span> <span class="n">non_trainable_variables</span>
        <span class="p">)</span>
        <span class="n">mapping</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">trainable_mapping</span><span class="p">)</span> <span class="o">+</span> <span class="nb">list</span><span class="p">(</span><span class="n">non_trainable_mapping</span><span class="p">)</span>

        <span class="c1"># Call in stateless scope</span>
        <span class="n">losses</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">with</span> <span class="n">backend</span><span class="o">.</span><span class="n">StatelessScope</span><span class="p">(</span>
            <span class="n">state_mapping</span><span class="o">=</span><span class="n">mapping</span><span class="p">,</span> <span class="n">collect_losses</span><span class="o">=</span><span class="n">return_losses</span>
        <span class="p">)</span> <span class="k">as</span> <span class="n">scope</span><span class="p">:</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">dtype_policy</span><span class="o">.</span><span class="n">quantization_mode</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">quantized_call</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">call</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">return_losses</span><span class="p">:</span>
                <span class="n">losses</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">losses</span>

        <span class="c1"># Gather updated non-trainable variables</span>
        <span class="n">non_trainable_variables</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">v</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">non_trainable_variables</span><span class="p">:</span>
            <span class="n">new_v</span> <span class="o">=</span> <span class="n">scope</span><span class="o">.</span><span class="n">get_current_value</span><span class="p">(</span><span class="n">v</span><span class="p">)</span>
            <span class="n">non_trainable_variables</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">new_v</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">return_losses</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">outputs</span><span class="p">,</span> <span class="n">non_trainable_variables</span><span class="p">,</span> <span class="n">losses</span>
        <span class="k">return</span> <span class="n">outputs</span><span class="p">,</span> <span class="n">non_trainable_variables</span>

    <span class="k">def</span> <span class="nf">compute_output_spec</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">utils</span><span class="o">.</span><span class="n">is_default</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">compute_output_shape</span><span class="p">):</span>
            <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">compute_output_spec</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># Use compute_output_shape() to return the right output spec</span>
            <span class="n">call_spec</span> <span class="o">=</span> <span class="n">CallSpec</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_call_signature</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">)</span>
            <span class="n">shapes_dict</span> <span class="o">=</span> <span class="n">get_shapes_dict</span><span class="p">(</span><span class="n">call_spec</span><span class="p">)</span>
            <span class="n">shapes_dict</span> <span class="o">=</span> <span class="n">update_shapes_dict_for_target_fn</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">compute_output_shape</span><span class="p">,</span>
                <span class="n">shapes_dict</span><span class="o">=</span><span class="n">shapes_dict</span><span class="p">,</span>
                <span class="n">call_spec</span><span class="o">=</span><span class="n">call_spec</span><span class="p">,</span>
                <span class="n">class_name</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="n">output_shape</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">compute_output_shape</span><span class="p">(</span><span class="o">**</span><span class="n">shapes_dict</span><span class="p">)</span>

            <span class="k">if</span> <span class="p">(</span>
                <span class="nb">isinstance</span><span class="p">(</span><span class="n">output_shape</span><span class="p">,</span> <span class="nb">list</span><span class="p">)</span>
                <span class="ow">and</span> <span class="n">output_shape</span>
                <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">output_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="p">(</span><span class="nb">int</span><span class="p">,</span> <span class="nb">type</span><span class="p">(</span><span class="kc">None</span><span class="p">)))</span>
            <span class="p">):</span>
                <span class="n">output_shape</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">output_shape</span><span class="p">)</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">output_shape</span><span class="p">,</span> <span class="p">(</span><span class="nb">list</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">,</span> <span class="nb">dict</span><span class="p">)):</span>
                <span class="k">try</span><span class="p">:</span>
                    <span class="n">output_shape</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">output_shape</span><span class="p">)</span>
                <span class="k">except</span><span class="p">:</span>
                    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                        <span class="s2">&quot;Method `compute_output_shape()` of layer &quot;</span>
                        <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2"> is returning &quot;</span>
                        <span class="s2">&quot;a type that cannot be interpreted as a shape. &quot;</span>
                        <span class="s2">&quot;It should return a shape tuple. &quot;</span>
                        <span class="sa">f</span><span class="s2">&quot;Received: </span><span class="si">{</span><span class="n">output_shape</span><span class="si">}</span><span class="s2">&quot;</span>
                    <span class="p">)</span>
            <span class="k">if</span> <span class="p">(</span>
                <span class="nb">isinstance</span><span class="p">(</span><span class="n">output_shape</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">)</span>
                <span class="ow">and</span> <span class="n">output_shape</span>
                <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">output_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="p">(</span><span class="nb">int</span><span class="p">,</span> <span class="nb">type</span><span class="p">(</span><span class="kc">None</span><span class="p">)))</span>
            <span class="p">):</span>
                <span class="k">return</span> <span class="n">KerasTensor</span><span class="p">(</span><span class="n">output_shape</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">compute_dtype</span><span class="p">)</span>
            <span class="c1"># Case: nested. Could be a tuple/list of shapes, or a dict of</span>
            <span class="c1"># shapes. Could be deeply nested.</span>
            <span class="k">return</span> <span class="n">tree</span><span class="o">.</span><span class="n">map_shape_structure</span><span class="p">(</span>
                <span class="k">lambda</span> <span class="n">s</span><span class="p">:</span> <span class="n">KerasTensor</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">compute_dtype</span><span class="p">),</span> <span class="n">output_shape</span>
            <span class="p">)</span>

    <span class="nd">@utils</span><span class="o">.</span><span class="n">default</span>
    <span class="k">def</span> <span class="nf">compute_output_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="k">raise</span> <span class="bp">self</span><span class="o">.</span><span class="n">_not_implemented_error</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">compute_output_shape</span><span class="p">,</span>
            <span class="s2">&quot;Should implement `def compute_output_shape(self, input_shape)`.&quot;</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">add_loss</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">loss</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Can be called inside of the `call()` method to add a scalar loss.</span>

<span class="sd">        Example:</span>

<span class="sd">        ```python</span>
<span class="sd">        class MyLayer(Layer):</span>
<span class="sd">            ...</span>
<span class="sd">            def call(self, x):</span>
<span class="sd">                self.add_loss(ops.sum(x))</span>
<span class="sd">                return x</span>
<span class="sd">        ```</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Eager only.</span>
        <span class="n">losses</span> <span class="o">=</span> <span class="n">tree</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">losses</span><span class="p">:</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">backend</span><span class="o">.</span><span class="n">is_tensor</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="s2">&quot;`add_loss()` can only be called from inside `build()` or &quot;</span>
                    <span class="sa">f</span><span class="s2">&quot;`call()`, on a tensor input. Received invalid value: </span><span class="si">{</span><span class="n">x</span><span class="si">}</span><span class="s2">&quot;</span>
                <span class="p">)</span>
        <span class="k">if</span> <span class="n">backend</span><span class="o">.</span><span class="n">in_stateless_scope</span><span class="p">():</span>
            <span class="n">scope</span> <span class="o">=</span> <span class="n">backend</span><span class="o">.</span><span class="n">get_stateless_scope</span><span class="p">()</span>
            <span class="k">if</span> <span class="n">scope</span><span class="o">.</span><span class="n">collect_losses</span><span class="p">:</span>
                <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">losses</span><span class="p">:</span>
                    <span class="n">scope</span><span class="o">.</span><span class="n">add_loss</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">_loss_ids</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="nb">id</span><span class="p">(</span><span class="n">loss</span><span class="p">))</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_losses</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">losses</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_get_own_losses</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">backend</span><span class="o">.</span><span class="n">in_stateless_scope</span><span class="p">():</span>
            <span class="n">losses</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="n">scope</span> <span class="o">=</span> <span class="n">backend</span><span class="o">.</span><span class="n">get_stateless_scope</span><span class="p">()</span>
            <span class="k">for</span> <span class="n">loss</span> <span class="ow">in</span> <span class="n">scope</span><span class="o">.</span><span class="n">losses</span><span class="p">:</span>
                <span class="k">if</span> <span class="nb">id</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_loss_ids</span><span class="p">:</span>
                    <span class="n">losses</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">losses</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_losses</span><span class="p">[:]</span>

    <span class="k">def</span> <span class="nf">_get_regularization_losses</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">weight_regularization_losses</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">variable</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">trainable_weights</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">variable</span><span class="o">.</span><span class="n">regularizer</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="k">continue</span>
            <span class="k">if</span> <span class="n">backend</span><span class="o">.</span><span class="n">in_stateless_scope</span><span class="p">()</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">in_symbolic_scope</span><span class="p">():</span>
                <span class="c1"># If in symbolic scope, we might get `None` from</span>
                <span class="c1"># `get_current_value` in `backend.compute_output_spec`. So we</span>
                <span class="c1"># assign `variable` instead.</span>
                <span class="n">v</span> <span class="o">=</span> <span class="n">backend</span><span class="o">.</span><span class="n">get_stateless_scope</span><span class="p">()</span><span class="o">.</span><span class="n">get_current_value</span><span class="p">(</span><span class="n">variable</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">v</span> <span class="o">=</span> <span class="n">variable</span>
            <span class="n">weight_regularization_losses</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">variable</span><span class="o">.</span><span class="n">regularizer</span><span class="p">(</span><span class="n">v</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">weight_regularization_losses</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">losses</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;List of scalar losses from `add_loss`, regularizers and sublayers.&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_losses_override</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_losses_override</span>
        <span class="n">losses</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_own_losses</span><span class="p">()</span>
        <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_flatten_layers</span><span class="p">(</span><span class="n">include_self</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
            <span class="n">losses</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">layer</span><span class="o">.</span><span class="n">_get_own_losses</span><span class="p">())</span>
        <span class="n">weight_regularization_losses</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_regularization_losses</span><span class="p">()</span>
        <span class="n">losses</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">weight_regularization_losses</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">losses</span>

    <span class="k">def</span> <span class="nf">_clear_losses</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">backend</span><span class="o">.</span><span class="n">in_stateless_scope</span><span class="p">():</span>
            <span class="n">scope</span> <span class="o">=</span> <span class="n">backend</span><span class="o">.</span><span class="n">get_stateless_scope</span><span class="p">()</span>
            <span class="k">if</span> <span class="n">scope</span><span class="o">.</span><span class="n">collect_losses</span><span class="p">:</span>
                <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">scope</span><span class="o">.</span><span class="n">losses</span><span class="p">:</span>
                    <span class="k">if</span> <span class="nb">id</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_loss_ids</span><span class="p">:</span>
                        <span class="n">scope</span><span class="o">.</span><span class="n">losses</span><span class="o">.</span><span class="n">remove</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_losses</span><span class="o">.</span><span class="n">clear</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_loss_ids</span><span class="o">.</span><span class="n">clear</span><span class="p">()</span>
        <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_layers</span><span class="p">:</span>
            <span class="n">layer</span><span class="o">.</span><span class="n">_clear_losses</span><span class="p">()</span>

    <span class="c1"># Quantization-related (int8 and float8) methods</span>

    <span class="k">def</span> <span class="nf">quantized_build</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_shape</span><span class="p">,</span> <span class="n">mode</span><span class="p">):</span>
        <span class="k">raise</span> <span class="bp">self</span><span class="o">.</span><span class="n">_not_implemented_error</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">quantized_build</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">quantize</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">mode</span><span class="p">,</span> <span class="n">type_check</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
        <span class="k">raise</span> <span class="bp">self</span><span class="o">.</span><span class="n">_not_implemented_error</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">quantize</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_check_quantize_args</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">mode</span><span class="p">,</span> <span class="n">compute_dtype</span><span class="p">):</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">built</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;Cannot quantize a layer that isn&#39;t yet built. &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;Layer &#39;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="si">}</span><span class="s2">&#39; (of type &#39;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2">&#39;) &quot;</span>
                <span class="s2">&quot;is not built yet.&quot;</span>
            <span class="p">)</span>
        <span class="k">if</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;_is_quantized&quot;</span><span class="p">,</span> <span class="kc">False</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;Layer &#39;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="si">}</span><span class="s2">&#39; is already quantized with &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;dtype_policy=&#39;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">dtype_policy</span><span class="o">.</span><span class="n">name</span><span class="si">}</span><span class="s2">&#39;. &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;Received: mode=</span><span class="si">{</span><span class="n">mode</span><span class="si">}</span><span class="s2">&quot;</span>
            <span class="p">)</span>
        <span class="k">if</span> <span class="n">mode</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">dtype_policies</span><span class="o">.</span><span class="n">QUANTIZATION_MODES</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;Invalid quantization mode. &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;Expected one of </span><span class="si">{</span><span class="n">dtype_policies</span><span class="o">.</span><span class="n">QUANTIZATION_MODES</span><span class="si">}</span><span class="s2">. &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;Received: mode=</span><span class="si">{</span><span class="n">mode</span><span class="si">}</span><span class="s2">&quot;</span>
            <span class="p">)</span>
        <span class="k">if</span> <span class="n">mode</span> <span class="o">==</span> <span class="s2">&quot;int8&quot;</span> <span class="ow">and</span> <span class="n">compute_dtype</span> <span class="o">==</span> <span class="s2">&quot;float16&quot;</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;Quantization mode=&#39;</span><span class="si">{</span><span class="n">mode</span><span class="si">}</span><span class="s2">&#39; doesn&#39;t work well with &quot;</span>
                <span class="s2">&quot;compute_dtype=&#39;float16&#39;. Consider loading model/layer with &quot;</span>
                <span class="s2">&quot;another dtype policy such as &#39;mixed_bfloat16&#39; or &quot;</span>
                <span class="s2">&quot;&#39;mixed_float16&#39; before calling `quantize()`.&quot;</span>
            <span class="p">)</span>

    <span class="k">def</span> <span class="nf">quantized_call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">quantization_mode</span> <span class="o">==</span> <span class="s2">&quot;int8&quot;</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_int8_call</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">quantization_mode</span> <span class="o">==</span> <span class="s2">&quot;float8&quot;</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_float8_call</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="bp">self</span><span class="o">.</span><span class="n">_quantization_mode_error</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">quantization_mode</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_int8_call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="k">raise</span> <span class="bp">self</span><span class="o">.</span><span class="n">_not_implemented_error</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_int8_call</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_float8_call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="k">raise</span> <span class="bp">self</span><span class="o">.</span><span class="n">_not_implemented_error</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_float8_call</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_not_implemented_error</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">attr</span><span class="p">,</span> <span class="n">msg</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="k">if</span> <span class="nb">callable</span><span class="p">(</span><span class="n">attr</span><span class="p">):</span>
            <span class="n">attr_name</span> <span class="o">=</span> <span class="n">attr</span><span class="o">.</span><span class="vm">__name__</span>
            <span class="n">attr_type</span> <span class="o">=</span> <span class="s2">&quot;method&quot;</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">attr_name</span> <span class="o">=</span> <span class="nb">str</span><span class="p">(</span><span class="n">attr</span><span class="p">)</span>
            <span class="n">attr_type</span> <span class="o">=</span> <span class="s2">&quot;attribute&quot;</span>
        <span class="n">msg</span> <span class="o">=</span> <span class="s2">&quot; &quot;</span> <span class="o">+</span> <span class="n">msg</span> <span class="k">if</span> <span class="n">msg</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="s2">&quot;&quot;</span>
        <span class="k">return</span> <span class="ne">NotImplementedError</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;Layer </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2"> does not have a `</span><span class="si">{</span><span class="n">attr_name</span><span class="si">}</span><span class="s2">` &quot;</span>
            <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">attr_type</span><span class="si">}</span><span class="s2"> implemented.</span><span class="si">{</span><span class="n">msg</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">_quantization_mode_error</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">mode</span><span class="p">):</span>
        <span class="k">return</span> <span class="ne">NotImplementedError</span><span class="p">(</span>
            <span class="s2">&quot;Invalid quantization mode. Expected one of &quot;</span>
            <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">dtype_policies</span><span class="o">.</span><span class="n">QUANTIZATION_MODES</span><span class="si">}</span><span class="s2">. &quot;</span>
            <span class="sa">f</span><span class="s2">&quot;Received: quantization_mode=</span><span class="si">{</span><span class="n">mode</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">save_own_variables</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">store</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Saves the state of the layer.</span>

<span class="sd">        You can override this method to take full control of how the state of</span>
<span class="sd">        the layer is saved upon calling `model.save()`.</span>

<span class="sd">        Args:</span>
<span class="sd">            store: Dict where the state of the model will be saved.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">all_vars</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_trainable_variables</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">_non_trainable_variables</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">all_vars</span><span class="p">):</span>
            <span class="n">store</span><span class="p">[</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">v</span>

    <span class="k">def</span> <span class="nf">load_own_variables</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">store</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Loads the state of the layer.</span>

<span class="sd">        You can override this method to take full control of how the state of</span>
<span class="sd">        the layer is loaded upon calling `keras.models.load_model()`.</span>

<span class="sd">        Args:</span>
<span class="sd">            store: Dict from which the state of the model will be loaded.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">all_vars</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_trainable_variables</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">_non_trainable_variables</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">store</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span> <span class="o">!=</span> <span class="nb">len</span><span class="p">(</span><span class="n">all_vars</span><span class="p">):</span>
            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">all_vars</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">and</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">built</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;Layer &#39;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="si">}</span><span class="s2">&#39; was never built &quot;</span>
                    <span class="s2">&quot;and thus it doesn&#39;t have any variables. &quot;</span>
                    <span class="sa">f</span><span class="s2">&quot;However the weights file lists </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">store</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span><span class="si">}</span><span class="s2"> &quot;</span>
                    <span class="s2">&quot;variables for this layer.</span><span class="se">\n</span><span class="s2">&quot;</span>
                    <span class="s2">&quot;In most cases, this error indicates that either:</span><span class="se">\n\n</span><span class="s2">&quot;</span>
                    <span class="s2">&quot;1. The layer is owned by a parent layer that &quot;</span>
                    <span class="s2">&quot;implements a `build()` method, but calling the &quot;</span>
                    <span class="s2">&quot;parent&#39;s `build()` method did NOT create the state of &quot;</span>
                    <span class="sa">f</span><span class="s2">&quot;the child layer &#39;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="si">}</span><span class="s2">&#39;. A `build()` method &quot;</span>
                    <span class="s2">&quot;must create ALL state for the layer, including &quot;</span>
                    <span class="s2">&quot;the state of any children layers.</span><span class="se">\n\n</span><span class="s2">&quot;</span>
                    <span class="s2">&quot;2. You need to implement &quot;</span>
                    <span class="s2">&quot;the `def build_from_config(self, config)` method &quot;</span>
                    <span class="sa">f</span><span class="s2">&quot;on layer &#39;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="si">}</span><span class="s2">&#39;, to specify how to rebuild &quot;</span>
                    <span class="s2">&quot;it during loading. &quot;</span>
                    <span class="s2">&quot;In this case, you might also want to implement the &quot;</span>
                    <span class="s2">&quot;method that generates the build config at saving time, &quot;</span>
                    <span class="s2">&quot;`def get_build_config(self)`. &quot;</span>
                    <span class="s2">&quot;The method `build_from_config()` is meant &quot;</span>
                    <span class="s2">&quot;to create the state &quot;</span>
                    <span class="s2">&quot;of the layer (i.e. its variables) upon deserialization.&quot;</span><span class="p">,</span>
                <span class="p">)</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;Layer &#39;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="si">}</span><span class="s2">&#39; expected </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">all_vars</span><span class="p">)</span><span class="si">}</span><span class="s2"> variables, &quot;</span>
                <span class="s2">&quot;but received &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">store</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span><span class="si">}</span><span class="s2"> variables during loading. &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;Expected: </span><span class="si">{</span><span class="p">[</span><span class="n">v</span><span class="o">.</span><span class="n">name</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">v</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="n">all_vars</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span>
            <span class="p">)</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">all_vars</span><span class="p">):</span>
            <span class="n">v</span><span class="o">.</span><span class="n">assign</span><span class="p">(</span><span class="n">store</span><span class="p">[</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">])</span>

    <span class="k">def</span> <span class="nf">_track_variable</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">variable</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">variable</span><span class="o">.</span><span class="n">trainable</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_tracker</span><span class="o">.</span><span class="n">add_to_store</span><span class="p">(</span><span class="s2">&quot;trainable_variables&quot;</span><span class="p">,</span> <span class="n">variable</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_tracker</span><span class="o">.</span><span class="n">add_to_store</span><span class="p">(</span><span class="s2">&quot;non_trainable_variables&quot;</span><span class="p">,</span> <span class="n">variable</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">trainable</span><span class="p">:</span>
            <span class="n">variable</span><span class="o">.</span><span class="n">trainable</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_post_track_variable</span><span class="p">(</span><span class="n">variable</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_untrack_variable</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">variable</span><span class="p">):</span>
        <span class="n">previous_lock_state</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_tracker</span><span class="o">.</span><span class="n">locked</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_tracker</span><span class="o">.</span><span class="n">unlock</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_tracker</span><span class="o">.</span><span class="n">untrack</span><span class="p">(</span><span class="n">variable</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">previous_lock_state</span> <span class="ow">is</span> <span class="kc">True</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_tracker</span><span class="o">.</span><span class="n">lock</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_post_untrack_variable</span><span class="p">(</span><span class="n">variable</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">add_metric</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="c1"># Permanently disabled</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span>
            <span class="s2">&quot;Layer `add_metric()` method is deprecated&quot;</span>
            <span class="s2">&quot; add your metric in `Model.compile(metrics=[...]).`&quot;</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">count_params</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Count the total number of scalars composing the weights.</span>

<span class="sd">        Returns:</span>
<span class="sd">            An integer count.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">built</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;You tried to call `count_params` &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;on layer &#39;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="si">}</span><span class="s2">&#39;, &quot;</span>
                <span class="s2">&quot;but the layer isn&#39;t built. &quot;</span>
                <span class="s2">&quot;You can build it manually via: &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;`layer.build(input_shape)`.&quot;</span>
            <span class="p">)</span>
        <span class="k">return</span> <span class="n">summary_utils</span><span class="o">.</span><span class="n">count_params</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_maybe_build</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">call_spec</span><span class="p">):</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">built</span><span class="p">:</span>
            <span class="k">return</span>

        <span class="n">shapes_dict</span> <span class="o">=</span> <span class="n">get_shapes_dict</span><span class="p">(</span><span class="n">call_spec</span><span class="p">)</span>
        <span class="n">first_shape</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="nb">iter</span><span class="p">(</span><span class="n">shapes_dict</span><span class="o">.</span><span class="n">values</span><span class="p">()),</span> <span class="kc">None</span><span class="p">)</span>

        <span class="c1"># If the layer has a build method, call it with our input shapes.</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">utils</span><span class="o">.</span><span class="n">is_default</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">build</span><span class="p">):</span>
            <span class="n">shapes_dict</span> <span class="o">=</span> <span class="n">update_shapes_dict_for_target_fn</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">build</span><span class="p">,</span>
                <span class="n">shapes_dict</span><span class="o">=</span><span class="n">shapes_dict</span><span class="p">,</span>
                <span class="n">call_spec</span><span class="o">=</span><span class="n">call_spec</span><span class="p">,</span>
                <span class="n">class_name</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">build</span><span class="p">(</span><span class="o">**</span><span class="n">shapes_dict</span><span class="p">)</span>
            <span class="c1"># Check input spec again (after build, since self.input_spec</span>
            <span class="c1"># may have been updated</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_assert_input_compatibility</span><span class="p">(</span><span class="n">call_spec</span><span class="o">.</span><span class="n">first_arg</span><span class="p">)</span>
            <span class="k">return</span>

        <span class="c1"># Otherwise, attempt to build the layer by calling it on symbolic input.</span>
        <span class="k">if</span> <span class="n">might_have_unbuilt_state</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
            <span class="k">try</span><span class="p">:</span>
                <span class="n">backend</span><span class="o">.</span><span class="n">compute_output_spec</span><span class="p">(</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">call</span><span class="p">,</span> <span class="o">**</span><span class="n">call_spec</span><span class="o">.</span><span class="n">arguments_dict</span>
                <span class="p">)</span>
            <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">call_spec</span><span class="o">.</span><span class="n">eager</span><span class="p">:</span>
                    <span class="c1"># Will let the actual eager call do state-building</span>
                    <span class="k">return</span>
                <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;Layer &#39;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="si">}</span><span class="s2">&#39; looks like it has unbuilt state, but &quot;</span>
                    <span class="s2">&quot;Keras is not able to trace the layer `call()` in order to &quot;</span>
                    <span class="s2">&quot;build it automatically. Possible causes:</span><span class="se">\n</span><span class="s2">&quot;</span>
                    <span class="s2">&quot;1. The `call()` method of your layer may be crashing. Try &quot;</span>
                    <span class="s2">&quot;to `__call__()` the layer eagerly on some test input &quot;</span>
                    <span class="s2">&quot;first to see if it works. &quot;</span>
                    <span class="s2">&quot;E.g. `x = np.random.random((3, 4)); y = layer(x)`</span><span class="se">\n</span><span class="s2">&quot;</span>
                    <span class="s2">&quot;2. If the `call()` method is correct, then you may need &quot;</span>
                    <span class="s2">&quot;to implement the `def build(self, input_shape)` method on &quot;</span>
                    <span class="s2">&quot;your layer. It should create all variables used by the &quot;</span>
                    <span class="s2">&quot;layer (e.g. by calling `layer.build()` on all its &quot;</span>
                    <span class="s2">&quot;children layers).</span><span class="se">\n</span><span class="s2">&quot;</span>
                    <span class="sa">f</span><span class="s2">&quot;Exception encountered: &#39;&#39;</span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s2">&#39;&#39;&quot;</span>
                <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">build</span><span class="p">(</span><span class="n">first_shape</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_build_by_run_for_single_pos_arg</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_shape</span><span class="p">):</span>
        <span class="c1"># Case: all inputs are in the first arg (possibly nested).</span>
        <span class="n">input_tensors</span> <span class="o">=</span> <span class="n">tree</span><span class="o">.</span><span class="n">map_shape_structure</span><span class="p">(</span>
            <span class="k">lambda</span> <span class="n">s</span><span class="p">:</span> <span class="n">backend</span><span class="o">.</span><span class="n">KerasTensor</span><span class="p">(</span><span class="n">s</span><span class="p">),</span> <span class="n">input_shape</span>
        <span class="p">)</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="n">backend</span><span class="o">.</span><span class="n">compute_output_spec</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">call</span><span class="p">,</span> <span class="n">input_tensors</span><span class="p">)</span>
            <span class="k">return</span> <span class="kc">True</span>
        <span class="k">except</span><span class="p">:</span>
            <span class="k">return</span> <span class="kc">False</span>

    <span class="k">def</span> <span class="nf">_build_by_run_for_kwargs</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">shapes_dict</span><span class="p">):</span>
        <span class="c1"># Case: inputs were recorded as multiple keyword arguments.</span>
        <span class="k">if</span> <span class="nb">all</span><span class="p">(</span><span class="n">is_shape_tuple</span><span class="p">(</span><span class="n">s</span><span class="p">)</span> <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">shapes_dict</span><span class="o">.</span><span class="n">values</span><span class="p">()):</span>
            <span class="c1"># Case: all input keyword arguments were plain tensors.</span>
            <span class="n">input_tensors</span> <span class="o">=</span> <span class="p">{</span>
                <span class="c1"># We strip the `_shape` suffix to recover kwarg names.</span>
                <span class="n">utils</span><span class="o">.</span><span class="n">removesuffix</span><span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="s2">&quot;_shape&quot;</span><span class="p">):</span> <span class="n">backend</span><span class="o">.</span><span class="n">KerasTensor</span><span class="p">(</span><span class="n">shape</span><span class="p">)</span>
                <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">shape</span> <span class="ow">in</span> <span class="n">shapes_dict</span><span class="o">.</span><span class="n">items</span><span class="p">()</span>
            <span class="p">}</span>
            <span class="k">try</span><span class="p">:</span>
                <span class="n">backend</span><span class="o">.</span><span class="n">compute_output_spec</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">call</span><span class="p">,</span> <span class="o">**</span><span class="n">input_tensors</span><span class="p">)</span>
                <span class="k">return</span> <span class="kc">True</span>
            <span class="k">except</span><span class="p">:</span>
                <span class="k">return</span> <span class="kc">False</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># Not supported: nested input keyword arguments.</span>
            <span class="k">return</span> <span class="kc">False</span>

    <span class="k">def</span> <span class="fm">__repr__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;&lt;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2"> &quot;</span>
            <span class="sa">f</span><span class="s2">&quot;name=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="si">}</span><span class="s2">, built=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">built</span><span class="si">}</span><span class="s2">&gt;&quot;</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="fm">__str__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="fm">__repr__</span><span class="p">()</span>

    <span class="k">def</span> <span class="fm">__setattr__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="c1"># Track Variables, Layers, Metrics, SeedGenerators.</span>
        <span class="n">name</span><span class="p">,</span> <span class="n">value</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_setattr_hook</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">value</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">name</span> <span class="o">!=</span> <span class="s2">&quot;_tracker&quot;</span><span class="p">:</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;_tracker&quot;</span><span class="p">):</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_initialize_tracker</span><span class="p">()</span>
            <span class="n">value</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_tracker</span><span class="o">.</span><span class="n">track</span><span class="p">(</span><span class="n">value</span><span class="p">)</span>
        <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__setattr__</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">value</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__delattr__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">):</span>
        <span class="n">obj</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">obj</span><span class="p">,</span> <span class="n">backend</span><span class="o">.</span><span class="n">Variable</span><span class="p">):</span>
            <span class="kn">import</span> <span class="nn">gc</span>

            <span class="c1"># It will take a short amount of time for the corresponding buffer</span>
            <span class="c1"># to be actually removed from the device.</span>
            <span class="c1"># https://stackoverflow.com/a/74631949</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_untrack_variable</span><span class="p">(</span><span class="n">obj</span><span class="p">)</span>
            <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__delattr__</span><span class="p">(</span><span class="n">name</span><span class="p">)</span>
            <span class="n">gc</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__delattr__</span><span class="p">(</span><span class="n">name</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_check_super_called</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">if</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;_lock&quot;</span><span class="p">,</span> <span class="kc">True</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;In layer &#39;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2">&#39;, you forgot to call &quot;</span>
                <span class="s2">&quot;`super().__init__()` as the first statement &quot;</span>
                <span class="s2">&quot;in the `__init__()` method. Go add it!&quot;</span>
            <span class="p">)</span>

    <span class="k">def</span> <span class="nf">_assert_input_compatibility</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">arg_0</span><span class="p">):</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">input_spec</span><span class="p">:</span>
            <span class="n">input_spec</span><span class="o">.</span><span class="n">assert_input_compatibility</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">input_spec</span><span class="p">,</span> <span class="n">arg_0</span><span class="p">,</span> <span class="n">layer_name</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">name</span>
            <span class="p">)</span>

    <span class="k">def</span> <span class="nf">_get_call_context</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Returns currently active `CallContext`.&quot;&quot;&quot;</span>
        <span class="n">layer_call_ctx</span> <span class="o">=</span> <span class="n">global_state</span><span class="o">.</span><span class="n">get_global_attribute</span><span class="p">(</span><span class="s2">&quot;current_call_ctx&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">layer_call_ctx</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="c1"># Enter new call context.</span>
            <span class="n">layer_call_ctx</span> <span class="o">=</span> <span class="n">CallContext</span><span class="p">(</span><span class="n">entry_layer</span><span class="o">=</span><span class="bp">self</span><span class="p">)</span>
            <span class="n">global_state</span><span class="o">.</span><span class="n">set_global_attribute</span><span class="p">(</span>
                <span class="s2">&quot;current_call_ctx&quot;</span><span class="p">,</span> <span class="n">layer_call_ctx</span>
            <span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_clear_losses</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">layer_call_ctx</span>

    <span class="k">def</span> <span class="nf">_maybe_reset_call_context</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">layer_call_ctx</span> <span class="o">=</span> <span class="n">global_state</span><span class="o">.</span><span class="n">get_global_attribute</span><span class="p">(</span><span class="s2">&quot;current_call_ctx&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">layer_call_ctx</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">layer_call_ctx</span><span class="o">.</span><span class="n">entry_layer</span> <span class="o">==</span> <span class="bp">self</span><span class="p">:</span>
            <span class="n">global_state</span><span class="o">.</span><span class="n">set_global_attribute</span><span class="p">(</span><span class="s2">&quot;current_call_ctx&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_flatten_layers</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">include_self</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">recursive</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
        <span class="n">layers</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">if</span> <span class="n">include_self</span><span class="p">:</span>
            <span class="n">layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
        <span class="n">seen_object_ids</span> <span class="o">=</span> <span class="nb">set</span><span class="p">()</span>
        <span class="n">deque</span> <span class="o">=</span> <span class="n">collections</span><span class="o">.</span><span class="n">deque</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_layers</span><span class="p">)</span>
        <span class="k">while</span> <span class="n">deque</span><span class="p">:</span>
            <span class="n">layer</span> <span class="o">=</span> <span class="n">deque</span><span class="o">.</span><span class="n">popleft</span><span class="p">()</span>
            <span class="k">if</span> <span class="nb">id</span><span class="p">(</span><span class="n">layer</span><span class="p">)</span> <span class="ow">in</span> <span class="n">seen_object_ids</span><span class="p">:</span>
                <span class="k">continue</span>
            <span class="n">seen_object_ids</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="nb">id</span><span class="p">(</span><span class="n">layer</span><span class="p">))</span>
            <span class="n">layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">layer</span><span class="p">)</span>
            <span class="c1"># Introspect recursively through sublayers.</span>
            <span class="k">if</span> <span class="n">recursive</span><span class="p">:</span>
                <span class="n">deque</span><span class="o">.</span><span class="n">extendleft</span><span class="p">(</span><span class="n">layer</span><span class="o">.</span><span class="n">_layers</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">layers</span>

    <span class="k">def</span> <span class="nf">_set_mask_metadata</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">outputs</span><span class="p">,</span> <span class="n">previous_mask</span><span class="p">):</span>
        <span class="n">flat_outputs</span> <span class="o">=</span> <span class="n">tree</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="n">outputs</span><span class="p">)</span>

        <span class="n">mask_already_computed</span> <span class="o">=</span> <span class="nb">all</span><span class="p">(</span>
            <span class="nb">getattr</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="s2">&quot;_keras_mask&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">flat_outputs</span>
        <span class="p">)</span>
        <span class="k">if</span> <span class="n">mask_already_computed</span><span class="p">:</span>
            <span class="k">return</span>

        <span class="n">output_masks</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">compute_mask</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">previous_mask</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">output_masks</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">return</span>

        <span class="n">flat_masks</span> <span class="o">=</span> <span class="n">tree</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="n">output_masks</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">tensor</span><span class="p">,</span> <span class="n">mask</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">flat_outputs</span><span class="p">,</span> <span class="n">flat_masks</span><span class="p">):</span>
            <span class="k">if</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="s2">&quot;_keras_mask&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="k">try</span><span class="p">:</span>
                    <span class="c1"># Numpy backend does not support masking.</span>
                    <span class="k">if</span> <span class="n">backend</span><span class="o">.</span><span class="n">backend</span><span class="p">()</span> <span class="o">==</span> <span class="s2">&quot;numpy&quot;</span><span class="p">:</span>
                        <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
                            <span class="s2">&quot;The NumPy backend does not support masking at this&quot;</span>
                            <span class="s2">&quot;time. Masks will be ignored.&quot;</span>
                        <span class="p">)</span>
                    <span class="n">tensor</span><span class="o">.</span><span class="n">_keras_mask</span> <span class="o">=</span> <span class="n">mask</span>
                <span class="k">except</span> <span class="ne">AttributeError</span><span class="p">:</span>
                    <span class="c1"># It&#39;s a C type.</span>
                    <span class="k">pass</span>

    <span class="nd">@python_utils</span><span class="o">.</span><span class="n">default</span>
    <span class="k">def</span> <span class="nf">get_config</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_check_super_called</span><span class="p">()</span>
        <span class="n">base_config</span> <span class="o">=</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">get_config</span><span class="p">()</span>
        <span class="n">config</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s2">&quot;trainable&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">trainable</span><span class="p">,</span>
            <span class="s2">&quot;dtype&quot;</span><span class="p">:</span> <span class="n">dtype_policies</span><span class="o">.</span><span class="n">serialize</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dtype_policy</span><span class="p">),</span>
        <span class="p">}</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">activity_regularizer</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">config</span><span class="p">[</span><span class="s2">&quot;activity_regularizer&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">regularizers</span><span class="o">.</span><span class="n">serialize</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">activity_regularizer</span>
            <span class="p">)</span>
        <span class="k">return</span> <span class="p">{</span><span class="o">**</span><span class="n">base_config</span><span class="p">,</span> <span class="o">**</span><span class="n">config</span><span class="p">}</span>

    <span class="k">def</span> <span class="nf">_open_name_scope</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_parent_path</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_parent_path</span> <span class="o">=</span> <span class="n">current_path</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">backend</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">caller</span><span class="o">=</span><span class="bp">self</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">is_backend_tensor_or_symbolic</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">allow_none</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">allow_none</span> <span class="ow">and</span> <span class="n">x</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">return</span> <span class="kc">True</span>
    <span class="k">return</span> <span class="n">backend</span><span class="o">.</span><span class="n">is_tensor</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="ow">or</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">backend</span><span class="o">.</span><span class="n">KerasTensor</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">CallSpec</span><span class="p">:</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">signature</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">):</span>
        <span class="c1"># `training` and `mask` are special kwargs that are always available in</span>
        <span class="c1"># a layer, if user specifies them in their call without adding to spec,</span>
        <span class="c1"># we remove them to be able to bind variables. User is not using</span>
        <span class="c1"># `training` anyway so we can ignore.</span>
        <span class="c1"># TODO: If necessary use workaround for `mask`</span>
        <span class="k">if</span> <span class="s2">&quot;training&quot;</span> <span class="ow">in</span> <span class="n">kwargs</span> <span class="ow">and</span> <span class="s2">&quot;training&quot;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">signature</span><span class="o">.</span><span class="n">parameters</span><span class="p">:</span>
            <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;training&quot;</span><span class="p">)</span>
            <span class="n">bound_args</span> <span class="o">=</span> <span class="n">signature</span><span class="o">.</span><span class="n">bind</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">bound_args</span> <span class="o">=</span> <span class="n">signature</span><span class="o">.</span><span class="n">bind</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">user_arguments_dict</span> <span class="o">=</span> <span class="p">{</span>
            <span class="n">k</span><span class="p">:</span> <span class="n">v</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">bound_args</span><span class="o">.</span><span class="n">arguments</span><span class="o">.</span><span class="n">items</span><span class="p">()</span>
        <span class="p">}</span>
        <span class="n">bound_args</span><span class="o">.</span><span class="n">apply_defaults</span><span class="p">()</span>
        <span class="n">arg_dict</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="n">arg_names</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">tensor_arg_dict</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="n">tensor_args</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">tensor_arg_names</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">nested_tensor_arg_names</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">bound_args</span><span class="o">.</span><span class="n">arguments</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="n">arg_dict</span><span class="p">[</span><span class="n">name</span><span class="p">]</span> <span class="o">=</span> <span class="n">value</span>
            <span class="n">arg_names</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">name</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">is_backend_tensor_or_symbolic</span><span class="p">(</span><span class="n">value</span><span class="p">):</span>
                <span class="n">tensor_args</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">value</span><span class="p">)</span>
                <span class="n">tensor_arg_names</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">name</span><span class="p">)</span>
                <span class="n">tensor_arg_dict</span><span class="p">[</span><span class="n">name</span><span class="p">]</span> <span class="o">=</span> <span class="n">value</span>
            <span class="k">elif</span> <span class="n">tree</span><span class="o">.</span><span class="n">is_nested</span><span class="p">(</span><span class="n">value</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="n">value</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                <span class="n">flat_values</span> <span class="o">=</span> <span class="n">tree</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="n">value</span><span class="p">)</span>
                <span class="k">if</span> <span class="nb">all</span><span class="p">(</span>
                    <span class="n">is_backend_tensor_or_symbolic</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">allow_none</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
                    <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">flat_values</span>
                <span class="p">):</span>
                    <span class="n">tensor_args</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">value</span><span class="p">)</span>
                    <span class="n">tensor_arg_names</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">name</span><span class="p">)</span>
                    <span class="n">tensor_arg_dict</span><span class="p">[</span><span class="n">name</span><span class="p">]</span> <span class="o">=</span> <span class="n">value</span>
                    <span class="n">nested_tensor_arg_names</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">name</span><span class="p">)</span>
                <span class="k">elif</span> <span class="nb">any</span><span class="p">(</span><span class="n">is_backend_tensor_or_symbolic</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">flat_values</span><span class="p">):</span>
                    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                        <span class="s2">&quot;In a nested call() argument, &quot;</span>
                        <span class="s2">&quot;you cannot mix tensors and non-tensors. &quot;</span>
                        <span class="s2">&quot;Received invalid mixed argument: &quot;</span>
                        <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">name</span><span class="si">}</span><span class="s2">=</span><span class="si">{</span><span class="n">value</span><span class="si">}</span><span class="s2">&quot;</span>
                    <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">arguments_dict</span> <span class="o">=</span> <span class="n">arg_dict</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">argument_names</span> <span class="o">=</span> <span class="n">arg_names</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">tensor_arguments_dict</span> <span class="o">=</span> <span class="n">tensor_arg_dict</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">tensor_arguments_names</span> <span class="o">=</span> <span class="n">tensor_arg_names</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">nested_tensor_argument_names</span> <span class="o">=</span> <span class="n">nested_tensor_arg_names</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">first_arg</span> <span class="o">=</span> <span class="n">arg_dict</span><span class="p">[</span><span class="n">arg_names</span><span class="p">[</span><span class="mi">0</span><span class="p">]]</span>
        <span class="k">if</span> <span class="nb">all</span><span class="p">(</span>
            <span class="n">backend</span><span class="o">.</span><span class="n">is_tensor</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">tensor_arguments_dict</span><span class="o">.</span><span class="n">values</span><span class="p">()</span>
        <span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">eager</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">eager</span> <span class="o">=</span> <span class="kc">False</span>


<span class="k">def</span> <span class="nf">get_arguments_dict</span><span class="p">(</span><span class="n">fn</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Return a dict mapping argument names to their values.&quot;&quot;&quot;</span>
    <span class="n">sig</span> <span class="o">=</span> <span class="n">inspect</span><span class="o">.</span><span class="n">signature</span><span class="p">(</span><span class="n">fn</span><span class="p">)</span>
    <span class="n">bound_args</span> <span class="o">=</span> <span class="n">sig</span><span class="o">.</span><span class="n">bind</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
    <span class="n">arg_dict</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">bound_args</span><span class="o">.</span><span class="n">arguments</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
        <span class="n">arg_dict</span><span class="p">[</span><span class="n">name</span><span class="p">]</span> <span class="o">=</span> <span class="n">value</span>
    <span class="k">return</span> <span class="n">arg_dict</span>


<span class="k">def</span> <span class="nf">get_shapes_dict</span><span class="p">(</span><span class="n">call_spec</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Convert the call() arguments dict into a dict of input shape arguments.</span>

<span class="sd">    Example:</span>

<span class="sd">    ```</span>
<span class="sd">    &gt;&gt;&gt; get_shapes_dict(call_spec)</span>
<span class="sd">    {&quot;input_a_shape&quot;: (2, 3)}</span>
<span class="sd">    ```</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">shapes_dict</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">call_spec</span><span class="o">.</span><span class="n">tensor_arguments_dict</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
        <span class="k">if</span> <span class="n">k</span> <span class="o">==</span> <span class="s2">&quot;mask&quot;</span> <span class="ow">or</span> <span class="n">k</span><span class="o">.</span><span class="n">endswith</span><span class="p">(</span><span class="s2">&quot;_mask&quot;</span><span class="p">):</span>
            <span class="c1"># Do not include mask tensors in shapes dict</span>
            <span class="k">continue</span>
        <span class="k">if</span> <span class="n">k</span> <span class="o">==</span> <span class="s2">&quot;kwargs&quot;</span> <span class="ow">or</span> <span class="n">k</span> <span class="o">==</span> <span class="s2">&quot;args&quot;</span><span class="p">:</span>
            <span class="c1"># Do not include catch-alls in shapes dict</span>
            <span class="k">continue</span>
        <span class="k">if</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">call_spec</span><span class="o">.</span><span class="n">nested_tensor_argument_names</span><span class="p">:</span>
            <span class="n">shapes_dict</span><span class="p">[</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">k</span><span class="si">}</span><span class="s2">_shape&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">tree</span><span class="o">.</span><span class="n">map_structure</span><span class="p">(</span>
                <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">backend</span><span class="o">.</span><span class="n">standardize_shape</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">),</span> <span class="n">v</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">shapes_dict</span><span class="p">[</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">k</span><span class="si">}</span><span class="s2">_shape&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">backend</span><span class="o">.</span><span class="n">standardize_shape</span><span class="p">(</span><span class="n">v</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">shapes_dict</span>


<span class="k">def</span> <span class="nf">update_shapes_dict_for_target_fn</span><span class="p">(</span>
    <span class="n">target_fn</span><span class="p">,</span>
    <span class="n">shapes_dict</span><span class="p">,</span>
    <span class="n">call_spec</span><span class="p">,</span>
    <span class="n">class_name</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Updates a `shapes_dict` for `build()` or `compute_output_shape()`.</span>

<span class="sd">    This function will align a dictionary of the shapes of all tensor</span>
<span class="sd">    passed to `call`, with the signatures of `build()` or</span>
<span class="sd">    `compute_output_shape()`.</span>

<span class="sd">    The alignment is a follows:</span>

<span class="sd">    - If `build()` or `compute_output_shape()` accept only one argument,</span>
<span class="sd">        forward the shape of the first positional argument from call without</span>
<span class="sd">        checking any argument names.</span>
<span class="sd">    - If `build()` or `compute_output_shape()` accept multiple arguments,</span>
<span class="sd">        enforce that all argument names match a call argument name, e.g.</span>
<span class="sd">        `foo_shape` would match call argument `foo`.</span>

<span class="sd">    Returns:</span>
<span class="sd">        An updated `shapes_dict` that can be used to invoke</span>
<span class="sd">        `target_fn(**shapes_dict)`.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">utils</span><span class="o">.</span><span class="n">is_default</span><span class="p">(</span><span class="n">target_fn</span><span class="p">):</span>
        <span class="k">return</span> <span class="kc">None</span>
    <span class="n">sig</span> <span class="o">=</span> <span class="n">inspect</span><span class="o">.</span><span class="n">signature</span><span class="p">(</span><span class="n">target_fn</span><span class="p">)</span>
    <span class="n">expected_names</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">sig</span><span class="o">.</span><span class="n">parameters</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
        <span class="k">if</span> <span class="n">param</span><span class="o">.</span><span class="n">kind</span> <span class="ow">in</span> <span class="p">(</span>
            <span class="n">param</span><span class="o">.</span><span class="n">POSITIONAL_OR_KEYWORD</span><span class="p">,</span>
            <span class="n">param</span><span class="o">.</span><span class="n">POSITIONAL_ONLY</span><span class="p">,</span>
            <span class="n">param</span><span class="o">.</span><span class="n">KEYWORD_ONLY</span><span class="p">,</span>
        <span class="p">):</span>
            <span class="n">expected_names</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">name</span><span class="p">)</span>

    <span class="c1"># Single arg: don&#39;t check names, pass first shape.</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">expected_names</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">key</span> <span class="o">=</span> <span class="n">expected_names</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">values</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">shapes_dict</span><span class="o">.</span><span class="n">values</span><span class="p">())</span>
        <span class="k">if</span> <span class="n">values</span><span class="p">:</span>
            <span class="n">input_shape</span> <span class="o">=</span> <span class="n">values</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">input_shape</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">return</span> <span class="p">{</span><span class="n">key</span><span class="p">:</span> <span class="n">input_shape</span><span class="p">}</span>

    <span class="c1"># Multiple args: check that all names line up.</span>
    <span class="n">kwargs</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="k">for</span> <span class="n">name</span> <span class="ow">in</span> <span class="n">expected_names</span><span class="p">:</span>
        <span class="n">method_name</span> <span class="o">=</span> <span class="n">target_fn</span><span class="o">.</span><span class="vm">__name__</span>
        <span class="n">error_preamble</span> <span class="o">=</span> <span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;For a `</span><span class="si">{</span><span class="n">method_name</span><span class="si">}</span><span class="s2">()` method with more than one argument, all &quot;</span>
            <span class="s2">&quot;arguments should have a `_shape` suffix and match an argument &quot;</span>
            <span class="sa">f</span><span class="s2">&quot;from `call()`. E.g. `</span><span class="si">{</span><span class="n">method_name</span><span class="si">}</span><span class="s2">(self, foo_shape, bar_shape)` &quot;</span>
        <span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">name</span><span class="o">.</span><span class="n">endswith</span><span class="p">(</span><span class="s2">&quot;_shape&quot;</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">error_preamble</span><span class="si">}</span><span class="s2"> For layer &#39;</span><span class="si">{</span><span class="n">class_name</span><span class="si">}</span><span class="s2">&#39;, &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;Received `</span><span class="si">{</span><span class="n">method_name</span><span class="si">}</span><span class="s2">()` argument &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;`</span><span class="si">{</span><span class="n">name</span><span class="si">}</span><span class="s2">`, which does not end in `_shape`.&quot;</span>
            <span class="p">)</span>
        <span class="n">expected_call_arg</span> <span class="o">=</span> <span class="n">utils</span><span class="o">.</span><span class="n">removesuffix</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="s2">&quot;_shape&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">expected_call_arg</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">call_spec</span><span class="o">.</span><span class="n">arguments_dict</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">error_preamble</span><span class="si">}</span><span class="s2"> For layer &#39;</span><span class="si">{</span><span class="n">class_name</span><span class="si">}</span><span class="s2">&#39;, &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;received `</span><span class="si">{</span><span class="n">method_name</span><span class="si">}</span><span class="s2">()` argument &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;`</span><span class="si">{</span><span class="n">name</span><span class="si">}</span><span class="s2">`, but `call()` does not have argument &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;`</span><span class="si">{</span><span class="n">expected_call_arg</span><span class="si">}</span><span class="s2">`.&quot;</span>
            <span class="p">)</span>
        <span class="k">if</span> <span class="n">name</span> <span class="ow">in</span> <span class="n">shapes_dict</span><span class="p">:</span>
            <span class="n">kwargs</span><span class="p">[</span><span class="n">name</span><span class="p">]</span> <span class="o">=</span> <span class="n">shapes_dict</span><span class="p">[</span><span class="n">name</span><span class="p">]</span>

    <span class="k">return</span> <span class="n">kwargs</span>


<span class="k">class</span> <span class="nc">CallContext</span><span class="p">:</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">entry_layer</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">entry_layer</span> <span class="o">=</span> <span class="n">entry_layer</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">training</span> <span class="o">=</span> <span class="kc">None</span>


<span class="k">def</span> <span class="nf">is_shape_tuple</span><span class="p">(</span><span class="n">s</span><span class="p">):</span>
    <span class="k">return</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="p">(</span><span class="nb">list</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">))</span> <span class="ow">and</span> <span class="nb">all</span><span class="p">(</span>
        <span class="n">d</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">d</span><span class="p">,</span> <span class="nb">int</span><span class="p">)</span> <span class="k">for</span> <span class="n">d</span> <span class="ow">in</span> <span class="n">s</span>
    <span class="p">)</span>


<span class="k">def</span> <span class="nf">might_have_unbuilt_state</span><span class="p">(</span><span class="n">layer</span><span class="p">):</span>
    <span class="k">return</span> <span class="nb">any</span><span class="p">(</span><span class="ow">not</span> <span class="n">lr</span><span class="o">.</span><span class="n">built</span> <span class="k">for</span> <span class="n">lr</span> <span class="ow">in</span> <span class="n">layer</span><span class="o">.</span><span class="n">_layers</span><span class="p">)</span>
</pre></div>

                </article>
              
              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
</div>
                </footer>
              
            </div>
            
            
              
            
          </div>
          <footer class="bd-footer-content">
            
          </footer>
        
      </main>
    </div>
  </div>
  
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../../../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  
<script type="module" src="../../../../_static/scripts/grg-sphinx-theme.js"></script>

  <footer class="bd-footer">
<div class="bd-footer__inner bd-page-width">
  
    <div class="footer-items__start">
      
        <div class="footer-item"><div>
  <h3 class="footer-heading">
    Never miss an update from us!
  </h3>
  <p class="footer-sub-heading">
    Don't worry! we are not going to spam you.
  </p>
  <div class="input-group mb-3 mar-t-10">
    <input type="text" class="form-control" placeholder="You email" aria-label="You email" id="grg-subscribe-email">
    <div class="input-group-append pointer" onclick="(document.getElementById('grg-subscribe-email').value)">
      <span class="input-group-text subscribe">Subscribe</span>
    </div>
  </div>  
  <ul class="navbar-icon-links"
    aria-label="Icon Links">
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://github.com/dipy" title="GitHub" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-github fa-lg" aria-hidden="true"></i>
            <span class="sr-only">GitHub</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://twitter.com/dipymri" title="Twitter/X" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-twitter fa-lg" aria-hidden="true"></i>
            <span class="sr-only">Twitter/X</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://www.youtube.com/c/diffusionimaginginpython" title="YouTube" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-youtube fa-lg" aria-hidden="true"></i>
            <span class="sr-only">YouTube</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://www.linkedin.com/company/dipy/" title="LinkedIn" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-linkedin fa-lg" aria-hidden="true"></i>
            <span class="sr-only">LinkedIn</span></a>
        </li>
</ul>
</div></div>
      
    </div>
  
  
  
    <div class="footer-items__end">
      
        <div class="footer-item"><div class="grid-3">
  
  <div>
    <h5 class="footer-heading">
      About
    </h5>
    <ul class="footer-section">
      
        
        <li class="pad-v-5">
          <a href="https://dipy.org/team">
            Developers
          </a>
        </li>
        
      
        
        <li class="pad-v-5">
          <a href="https://github.com/dipy/dipy/discussions" target="_blank" rel="noopener noreferrer">
            Support
            <i class="fa-solid fa-arrow-up-long mar-l-5"></i>
          </a>
        </li>
        
      
        
        <li class="pad-v-5">
          <a href="../../../../user_guide/installation.html">
            Download
          </a>
        </li>
        
      
        
        <li class="pad-v-5">
          <a href="../../../../user_guide/getting_started.html">
            Get Started
          </a>
        </li>
        
      
        
        <li class="pad-v-5">
          <a href="../../../../examples_built/index.html">
            Tutorials
          </a>
        </li>
        
      
        
        <li class="pad-v-5">
          <a href="https://www.youtube.com/c/diffusionimaginginpython" target="_blank" rel="noopener noreferrer">
            Videos
            <i class="fa-solid fa-arrow-up-long mar-l-5"></i>
          </a>
        </li>
        
      
    </ul>
  </div>
  
  <div>
    <h5 class="footer-heading">
      Friends
    </h5>
    <ul class="footer-section">
      
        
        <li class="pad-v-5">
          <a href="http://nipy.org/" target="_blank" rel="noopener noreferrer">
            Nipy Projects
            <i class="fa-solid fa-arrow-up-long mar-l-5"></i>
          </a>
        </li>
        
      
        
        <li class="pad-v-5">
          <a href="http://fury.gl/" target="_blank" rel="noopener noreferrer">
            FURY
            <i class="fa-solid fa-arrow-up-long mar-l-5"></i>
          </a>
        </li>
        
      
        
        <li class="pad-v-5">
          <a href="http://nipy.org/nibabel" target="_blank" rel="noopener noreferrer">
            Nibabel
            <i class="fa-solid fa-arrow-up-long mar-l-5"></i>
          </a>
        </li>
        
      
        
        <li class="pad-v-5">
          <a href="https://tortoise.nibib.nih.gov/" target="_blank" rel="noopener noreferrer">
            Tortoise
            <i class="fa-solid fa-arrow-up-long mar-l-5"></i>
          </a>
        </li>
        
      
    </ul>
  </div>
  
  <div>
    <h5 class="footer-heading">
      Support
    </h5>
    <ul class="footer-section">
      
        
        <li class="pad-v-5">
          <a href="https://engineering.indiana.edu/" target="_blank" rel="noopener noreferrer">
            The department of Intelligent Systems Engineering of Indiana University
            <i class="fa-solid fa-arrow-up-long mar-l-5"></i>
          </a>
        </li>
        
      
        
        <li class="pad-v-5">
          <a href="https://www.nibib.nih.gov/" target="_blank" rel="noopener noreferrer">
            The National Institute of Biomedical Imaging and Bioengineering, NIH
            <i class="fa-solid fa-arrow-up-long mar-l-5"></i>
          </a>
        </li>
        
      
        
        <li class="pad-v-5">
          <a href="https://escience.washington.edu" target="_blank" rel="noopener noreferrer">
            The Gordon and Betty Moore Foundation and the Alfred P. Sloan Foundation, through the University of Washington eScience Institute Data Science Environment
            <i class="fa-solid fa-arrow-up-long mar-l-5"></i>
          </a>
        </li>
        
      
        
        <li class="pad-v-5">
          <a href="https://summerofcode.withgoogle.com/" target="_blank" rel="noopener noreferrer">
            Google supported DIPY through the Google Summer of Code Program (2015-2024)
            <i class="fa-solid fa-arrow-up-long mar-l-5"></i>
          </a>
        </li>
        
      
    </ul>
  </div>
  
</div></div>
      
    </div>
  
</div>

    <div class="copyright-text">
      <i class="fa-regular fa-copyright"></i> Copyright 2008-2024,DIPY developers. Created using Grg Sphinx Theme and PyData Sphinx Theme.
    </div>
  </footer>
  </body>
</html>